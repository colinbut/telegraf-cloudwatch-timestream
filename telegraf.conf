# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply surround
# them with ${}. For strings the variable must be within quotes (ie, "${STR_VAR}"),
# for numbers and booleans they should be plain (ie, ${INT_VAR}, ${BOOL_VAR})


# Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Collection offset is used to shift the collection by the given amount.
  ## This can be be used to avoid many plugins querying constraint devices
  ## at the same time by manually scheduling them in time.
  # collection_offset = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## Collected metrics are rounded to the precision specified. Precision is
  ## specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s).
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  ##
  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s:
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ##
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  precision = "0s"

  ## Log at debug level.
  debug = true
  ## Log only error level messages.
  # quiet = false

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  # logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  # logfile = ""

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0h"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5

  ## Pick a timezone to use when logging or type 'local' for local time.
  ## Example: America/Chicago
  # log_with_timezone = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false

  ## Method of translating SNMP objects. Can be "netsnmp" (deprecated) which
  ## translates by calling external programs snmptranslate and snmptable,
  ## or "gosmi" which translates using the built-in gosmi library.
  # snmp_translator = "netsnmp"

  ## Name of the file to load the state of plugins from and store the state to.
  ## If uncommented and not empty, this file will be used to save the state of
  ## stateful plugins on termination of Telegraf. If the file exists on start,
  ## the state in the file will be restored for the plugins.
  # statefile = ""

###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################


# # Configuration for AWS CloudWatchLogs output.
 [[outputs.cloudwatch_logs]]
#   ## The region is the Amazon region that you wish to connect to.
#   ## Examples include but are not limited to:
#   ## - us-west-1
#   ## - us-west-2
#   ## - us-east-1
#   ## - ap-southeast-1
#   ## - ap-southeast-2
#   ## ...
   region = "eu-west-1"
#
#   ## Amazon Credentials
#   ## Credentials are loaded in the following order
#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified
#   ## 2) Assumed credentials via STS if role_arn is specified
#   ## 3) explicit credentials from 'access_key' and 'secret_key'
#   ## 4) shared profile from 'profile'
#   ## 5) environment variables
#   ## 6) shared credentials file
#   ## 7) EC2 Instance Profile
#   #access_key = ""
#   #secret_key = ""
#   #token = ""
#   #role_arn = ""
#   #web_identity_token_file = ""
#   #role_session_name = ""
   profile = "profile_name"
#   #shared_credential_file = ""
#
#   ## Endpoint to make request against, the correct endpoint is automatically
#   ## determined and this option should only be set if you wish to override the
#   ## default.
#   ##   ex: endpoint_url = "http://localhost:8000"
#   # endpoint_url = ""
#
#   ## Cloud watch log group. Must be created in AWS cloudwatch logs upfront!
#   ## For example, you can specify the name of the k8s cluster here to group logs from all cluster in oine place
   log_group = "log-group"
#
#   ## Log stream in log group
#   ## Either log group name or reference to metric attribute, from which it can be parsed:
#   ## tag:<TAG_NAME> or field:<FIELD_NAME>. If log stream is not exist, it will be created.
#   ## Since AWS is not automatically delete logs streams with expired logs entries (i.e. empty log stream)
#   ## you need to put in place appropriate house-keeping (https://forums.aws.amazon.com/thread.jspa?threadID=178855)
   log_stream = "log-group"
#
#   ## Source of log data - metric name
#   ## specify the name of the metric, from which the log data should be retrieved.
#   ## I.e., if you  are using docker_log plugin to stream logs from container, then
#   ## specify log_data_metric_name  = "docker_log"
   log_data_metric_name  = "docker_log"
#
#   ## Specify from which metric attribute the log data should be retrieved:
#   ## tag:<TAG_NAME> or field:<FIELD_NAME>.
#   ## I.e., if you  are using docker_log plugin to stream logs from container, then
#   ## specify log_data_source  = "field:message"
   log_data_source  = "field:message"


###############################################################################
#                            PROCESSOR PLUGINS                                #
###############################################################################


# # Attach AWS EC2 metadata to metrics
# [[processors.aws_ec2]]
#   ## Instance identity document tags to attach to metrics.
#   ## For more information see:
#   ## https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html
#   ##
#   ## Available tags:
#   ## * accountId
#   ## * architecture
#   ## * availabilityZone
#   ## * billingProducts
#   ## * imageId
#   ## * instanceId
#   ## * instanceType
#   ## * kernelId
#   ## * pendingTime
#   ## * privateIp
#   ## * ramdiskId
#   ## * region
#   ## * version
#   imds_tags = []
#
#   ## EC2 instance tags retrieved with DescribeTags action.
#   ## In case tag is empty upon retrieval it's omitted when tagging metrics.
#   ## Note that in order for this to work, role attached to EC2 instance or AWS
#   ## credentials available from the environment must have a policy attached, that
#   ## allows ec2:DescribeTags.
#   ##
#   ## For more information see:
#   ## https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeTags.html
#   ec2_tags = []
#
#   ## Timeout for http requests made by against aws ec2 metadata endpoint.
#   timeout = "10s"
#
#   ## ordered controls whether or not the metrics need to stay in the same order
#   ## this plugin received them in. If false, this plugin will change the order
#   ## with requests hitting cached results moving through immediately and not
#   ## waiting on slower lookups. This may cause issues for you if you are
#   ## depending on the order of metrics staying the same. If so, set this to true.
#   ## Keeping the metrics ordered may be slightly slower.
#   ordered = false
#
#   ## max_parallel_calls is the maximum number of AWS API calls to be in flight
#   ## at the same time.
#   ## It's probably best to keep this number fairly low.
#   max_parallel_calls = 10
#
#   ## cache_ttl determines how long each cached item will remain in the cache before
#   ## it is removed and subsequently needs to be queried for from the AWS API. By
#   ## default, no items are cached.
#   # cache_ttl = "0s"
#
#   ## tag_cache_size determines how many of the values which are found in imds_tags
#   ## or ec2_tags will be kept in memory for faster lookup on successive processing
#   ## of metrics. You may want to adjust this if you have excessively large numbers
#   ## of tags on your EC2 instances, and you are using the ec2_tags field. This
#   ## typically does not need to be changed when using the imds_tags field.
#   # tag_cache_size = 1000
#
#   ## log_cache_stats will emit a log line periodically to stdout with details of
#   ## cache entries, hits, misses, and evacuations since the last time stats were
#   ## emitted. This can be helpful in determining whether caching is being effective
#   ## in your environment. Stats are emitted every 30 seconds. By default, this
#   ## setting is disabled.


# # Apply metric modifications using override semantics.
# [[processors.clone]]
#   ## All modifications on inputs and aggregators can be overridden:
#   # name_override = "new_name"
#   # name_prefix = "new_name_prefix"
#   # name_suffix = "new_name_suffix"
#
#   ## Tags to be added (all values must be strings)
#   # [processors.clone.tags]
#   #   additional_tag = "tag_value"


# # Convert values to another metric value type
# [[processors.converter]]
#   ## Tags to convert
#   ##
#   ## The table key determines the target type, and the array of key-values
#   ## select the keys to convert.  The array may contain globs.
#   ##   <target-type> = [<tag-key>...]
#   [processors.converter.tags]
#     measurement = []
#     string = []
#     integer = []
#     unsigned = []
#     boolean = []
#     float = []
#
#     ## Optional tag to use as metric timestamp
#     # timestamp = []
#
#     ## Format of the timestamp determined by the tag above. This can be any of
#     ## "unix", "unix_ms", "unix_us", "unix_ns", or a valid Golang time format.
#     ## It is required, when using the timestamp option.
#     # timestamp_format = ""
#
#   ## Fields to convert
#   ##
#   ## The table key determines the target type, and the array of key-values
#   ## select the keys to convert.  The array may contain globs.
#   ##   <target-type> = [<field-key>...]
#   [processors.converter.fields]
#     measurement = []
#     tag = []
#     string = []
#     integer = []
#     unsigned = []
#     boolean = []
#     float = []
#
#     ## Optional field to use as metric timestamp
#     # timestamp = []
#
#     ## Format of the timestamp determined by the field above. This can be any
#     ## of "unix", "unix_ms", "unix_us", "unix_ns", or a valid Golang time
#     ## format. It is required, when using the timestamp option.
#     # timestamp_format = ""


# # Dates measurements, tags, and fields that pass through this filter.
# [[processors.date]]
#   ## New tag to create
#   tag_key = "month"
#
#   ## New field to create (cannot set both field_key and tag_key)
#   # field_key = "month"
#
#   ## Date format string, must be a representation of the Go "reference time"
#   ## which is "Mon Jan 2 15:04:05 -0700 MST 2006".
#   date_format = "Jan"
#
#   ## If destination is a field, date format can also be one of
#   ## "unix", "unix_ms", "unix_us", or "unix_ns", which will insert an integer field.
#   # date_format = "unix"
#
#   ## Offset duration added to the date string when writing the new tag.
#   # date_offset = "0s"
#
#   ## Timezone to use when creating the tag or field using a reference time
#   ## string.  This can be set to one of "UTC", "Local", or to a location name
#   ## in the IANA Time Zone database.
#   ##   example: timezone = "America/Los_Angeles"
#   # timezone = "UTC"


# # Filter metrics with repeating field values
# [[processors.dedup]]
#   ## Maximum time to suppress output
#   dedup_interval = "600s"


# ## Set default fields on your metric(s) when they are nil or empty
# [[processors.defaults]]
#   ## Ensures a set of fields always exists on your metric(s) with their
#   ## respective default value.
#   ## For any given field pair (key = default), if it's not set, a field
#   ## is set on the metric with the specified default.
#   ##
#   ## A field is considered not set if it is nil on the incoming metric;
#   ## or it is not nil but its value is an empty string or is a string
#   ## of one or more spaces.
#   ##   <target-field> = <value>
#   [processors.defaults.fields]
#     field_1 = "bar"
#     time_idle = 0
#     is_error = true


# # Map enum values according to given table.
# [[processors.enum]]
#   [[processors.enum.mapping]]
#     ## Name of the field to map. Globs accepted.
#     field = "status"
#
#     ## Name of the tag to map. Globs accepted.
#     # tag = "status"
#
#     ## Destination tag or field to be used for the mapped value.  By default the
#     ## source tag or field is used, overwriting the original value.
#     dest = "status_code"
#
#     ## Default value to be used for all values not contained in the mapping
#     ## table.  When unset and no match is found, the original field will remain
#     ## unmodified and the destination tag or field will not be created.
#     # default = 0
#
#     ## Table of mappings
#     [processors.enum.mapping.value_mappings]
#       green = 1
#       amber = 2
#       red = 3


# # Run executable as long-running processor plugin
# [[processors.execd]]
#   ## One program to run as daemon.
#   ## NOTE: process and each argument should each be their own string
#   ## eg: command = ["/path/to/your_program", "arg1", "arg2"]
#   command = ["cat"]
#
#   ## Environment variables
#   ## Array of "key=value" pairs to pass as environment variables
#   ## e.g. "KEY=value", "USERNAME=John Doe",
#   ## "LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs"
#   # environment = []
#
#   ## Delay before the process is restarted after an unexpected termination
#   # restart_delay = "10s"
#
#   ## Serialization format for communicating with the executed program
#   ## Please note that the corresponding data-format must exist both in
#   ## parsers and serializers
#   # data_format = "influx"


# # Performs file path manipulations on tags and fields
# [[processors.filepath]]
#   ## Treat the tag value as a path and convert it to its last element, storing the result in a new tag
#   # [[processors.filepath.basename]]
#   #   tag = "path"
#   #   dest = "basepath"
#
#   ## Treat the field value as a path and keep all but the last element of path, typically the path's directory
#   # [[processors.filepath.dirname]]
#   #   field = "path"
#
#   ## Treat the tag value as a path, converting it to its the last element without its suffix
#   # [[processors.filepath.stem]]
#   #   tag = "path"
#
#   ## Treat the tag value as a path, converting it to the shortest path name equivalent
#   ## to path by purely lexical processing
#   # [[processors.filepath.clean]]
#   #   tag = "path"
#
#   ## Treat the tag value as a path, converting it to a relative path that is lexically
#   ## equivalent to the source path when joined to 'base_path'
#   # [[processors.filepath.rel]]
#   #   tag = "path"
#   #   base_path = "/var/log"
#
#   ## Treat the tag value as a path, replacing each separator character in path with a '/' character. Has only
#   ## effect on Windows
#   # [[processors.filepath.toslash]]
#   #   tag = "path"


# # Add a tag of the network interface name looked up over SNMP by interface number
# [[processors.ifname]]
#   ## Name of tag holding the interface number
#   # tag = "ifIndex"
#
#   ## Name of output tag where service name will be added
#   # dest = "ifName"
#
#   ## Name of tag of the SNMP agent to request the interface name from
#   # agent = "agent"
#
#   ## Timeout for each request.
#   # timeout = "5s"
#
#   ## SNMP version; can be 1, 2, or 3.
#   # version = 2
#
#   ## SNMP community string.
#   # community = "public"
#
#   ## Number of retries to attempt.
#   # retries = 3
#
#   ## The GETBULK max-repetitions parameter.
#   # max_repetitions = 10
#
#   ## SNMPv3 authentication and encryption options.
#   ##
#   ## Security Name.
#   # sec_name = "myuser"
#   ## Authentication protocol; one of "MD5", "SHA", or "".
#   # auth_protocol = "MD5"
#   ## Authentication password.
#   # auth_password = "pass"
#   ## Security Level; one of "noAuthNoPriv", "authNoPriv", or "authPriv".
#   # sec_level = "authNoPriv"
#   ## Context Name.
#   # context_name = ""
#   ## Privacy protocol used for encrypted messages; one of "DES", "AES" or "".
#   # priv_protocol = ""
#   ## Privacy password used for encrypted messages.
#   # priv_password = ""
#
#   ## max_parallel_lookups is the maximum number of SNMP requests to
#   ## make at the same time.
#   # max_parallel_lookups = 100
#
#   ## ordered controls whether or not the metrics need to stay in the
#   ## same order this plugin received them in. If false, this plugin
#   ## may change the order when data is cached.  If you need metrics to
#   ## stay in order set this to true.  keeping the metrics ordered may
#   ## be slightly slower
#   # ordered = false
#
#   ## cache_ttl is the amount of time interface names are cached for a
#   ## given agent.  After this period elapses if names are needed they
#   ## will be retrieved again.
#   # cache_ttl = "8h"


# # Lookup a key derived from metrics in a static file
# [[processors.lookup]]
#   ## List of files containing the lookup-table
#   files = ["path/to/lut.json", "path/to/another_lut.json"]
#
#   ## Format of the lookup file(s)
#   ## Available formats are:
#   ##    json               -- JSON file with 'key: {tag-key: tag-value, ...}' mapping
#   ##    csv_key_name_value -- CSV file with 'key,tag-key,tag-value,...,tag-key,tag-value' mapping
#   ##    csv_key_values     -- CSV file with a header containing tag-names and
#   ##                          rows with 'key,tag-value,...,tag-value' mappings
#   # format = "json"
#
#   ## Template for generating the lookup-key from the metric.
#   ## This is a Golang template (see https://pkg.go.dev/text/template) to
#   ## access the metric name (`{{.Name}}`), a tag value (`{{.Tag "name"}}`) or
#   ## a field value (`{{.Field "name"}}`).
#   key = '{{.Tag "host"}}'


# # Adds noise to numerical fields
# [[processors.noise]]
#   ## Specified the type of the random distribution.
#   ## Can be "laplacian", "gaussian" or "uniform".
#   # type = "laplacian
#
#   ## Center of the distribution.
#   ## Only used for Laplacian and Gaussian distributions.
#   # mu = 0.0
#
#   ## Scale parameter for the Laplacian or Gaussian distribution
#   # scale = 1.0
#
#   ## Upper and lower bound of the Uniform distribution
#   # min = -1.0
#   # max = 1.0
#
#   ## Apply the noise only to numeric fields matching the filter criteria below.
#   ## Excludes takes precedence over includes.
#   # include_fields = []
#   # exclude_fields = []


# # Apply metric modifications using override semantics.
# [[processors.override]]
#   ## All modifications on inputs and aggregators can be overridden:
#   # name_override = "new_name"
#   # name_prefix = "new_name_prefix"
#   # name_suffix = "new_name_suffix"
#
#   ## Tags to be added (all values must be strings)
#   # [processors.override.tags]
#   #   additional_tag = "tag_value"


# # Parse a value in a specified field(s)/tag(s) and add the result in a new metric
# [[processors.parser]]
#   ## The name of the fields whose value will be parsed.
#   parse_fields = ["message"]
#
#   ## The name of the tags whose value will be parsed.
#   # parse_tags = []
#
#   ## If true, incoming metrics are not emitted.
#   # drop_original = false
#
#   ## Merge Behavior
#   ## Only has effect when drop_original is set to false. Possible options
#   ## include:
#   ##  * override: emitted metrics are merged by overriding the original metric
#   ##    using the newly parsed metrics, but retains the original metric
#   ##    timestamp.
#   ##  * override-with-timestamp: the same as "override", but the timestamp is
#   ##    set based on the new metrics if present.
#   # merge = ""
#
#   ## The dataformat to be read from files
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"


# # Rotate a single valued metric into a multi field metric
# [[processors.pivot]]
#   ## Tag to use for naming the new field.
#   tag_key = "name"
#   ## Field to use as the value of the new field.
#   value_key = "value"


# # Given a tag/field of a TCP or UDP port number, add a tag/field of the service name looked up in the system services file
# [[processors.port_name]]
#   ## Name of tag holding the port number
#   # tag = "port"
#   ## Or name of the field holding the port number
#   # field = "port"
#
#   ## Name of output tag or field (depending on the source) where service name will be added
#   # dest = "service"
#
#   ## Default tcp or udp
#   # default_protocol = "tcp"
#
#   ## Tag containing the protocol (tcp or udp, case-insensitive)
#   # protocol_tag = "proto"
#
#   ## Field containing the protocol (tcp or udp, case-insensitive)
#   # protocol_field = "proto"


# # Print all metrics that pass through this filter.
# [[processors.printer]]


# # Transforms tag and field values as well as measurement, tag and field names with regex pattern
# [[processors.regex]]
#   namepass = ["nginx_requests"]
#
#   # Tag and field conversions defined in a separate sub-tables
#   [[processors.regex.tags]]
#     ## Tag to change, "*" will change every tag
#     key = "resp_code"
#     ## Regular expression to match on a tag value
#     pattern = "^(\\d)\\d\\d$"
#     ## Matches of the pattern will be replaced with this string.  Use ${1}
#     ## notation to use the text of the first submatch.
#     replacement = "${1}xx"
#
#   [[processors.regex.fields]]
#     ## Field to change
#     key = "request"
#     ## All the power of the Go regular expressions available here
#     ## For example, named subgroups
#     pattern = "^/api(?P<method>/[\\w/]+)\\S*"
#     replacement = "${method}"
#     ## If result_key is present, a new field will be created
#     ## instead of changing existing field
#     result_key = "method"
#
#   # Multiple conversions may be applied for one field sequentially
#   # Let's extract one more value
#   [[processors.regex.fields]]
#     key = "request"
#     pattern = ".*category=(\\w+).*"
#     replacement = "${1}"
#     result_key = "search_category"
#
#   # Rename metric fields
#   [[processors.regex.field_rename]]
#     ## Regular expression to match on a field name
#     pattern = "^search_(\\w+)d$"
#     ## Matches of the pattern will be replaced with this string.  Use ${1}
#     ## notation to use the text of the first submatch.
#     replacement = "${1}"
#     ## If the new field name already exists, you can either "overwrite" the
#     ## existing one with the value of the renamed field OR you can "keep"
#     ## both the existing and source field.
#     # result_key = "keep"
#
#   # Rename metric tags
#   # [[processors.regex.tag_rename]]
#   #   ## Regular expression to match on a tag name
#   #   pattern = "^search_(\\w+)d$"
#   #   ## Matches of the pattern will be replaced with this string.  Use ${1}
#   #   ## notation to use the text of the first submatch.
#   #   replacement = "${1}"
#   #   ## If the new tag name already exists, you can either "overwrite" the
#   #   ## existing one with the value of the renamed tag OR you can "keep"
#   #   ## both the existing and source tag.
#   #   # result_key = "keep"
#
#   # Rename metrics
#   # [[processors.regex.metric_rename]]
#   #   ## Regular expression to match on an metric name
#   #   pattern = "^search_(\\w+)d$"
#   #   ## Matches of the pattern will be replaced with this string.  Use ${1}
#   #   ## notation to use the text of the first submatch.
#   #   replacement = "${1}"


# # Rename measurements, tags, and fields that pass through this filter.
# [[processors.rename]]
#   ## Specify one sub-table per rename operation.
#   [[processors.rename.replace]]
#     measurement = "network_interface_throughput"
#     dest = "throughput"
#
#   [[processors.rename.replace]]
#     tag = "hostname"
#     dest = "host"
#
#   [[processors.rename.replace]]
#     field = "lower"
#     dest = "min"
#
#   [[processors.rename.replace]]
#     field = "upper"
#     dest = "max"


# # ReverseDNS does a reverse lookup on IP addresses to retrieve the DNS name
# [[processors.reverse_dns]]
#   ## For optimal performance, you may want to limit which metrics are passed to this
#   ## processor. eg:
#   ## namepass = ["my_metric_*"]
#
#   ## cache_ttl is how long the dns entries should stay cached for.
#   ## generally longer is better, but if you expect a large number of diverse lookups
#   ## you'll want to consider memory use.
#   cache_ttl = "24h"
#
#   ## lookup_timeout is how long should you wait for a single dns request to repsond.
#   ## this is also the maximum acceptable latency for a metric travelling through
#   ## the reverse_dns processor. After lookup_timeout is exceeded, a metric will
#   ## be passed on unaltered.
#   ## multiple simultaneous resolution requests for the same IP will only make a
#   ## single rDNS request, and they will all wait for the answer for this long.
#   lookup_timeout = "3s"
#
#   ## max_parallel_lookups is the maximum number of dns requests to be in flight
#   ## at the same time. Requesting hitting cached values do not count against this
#   ## total, and neither do mulptiple requests for the same IP.
#   ## It's probably best to keep this number fairly low.
#   max_parallel_lookups = 10
#
#   ## ordered controls whether or not the metrics need to stay in the same order
#   ## this plugin received them in. If false, this plugin will change the order
#   ## with requests hitting cached results moving through immediately and not
#   ## waiting on slower lookups. This may cause issues for you if you are
#   ## depending on the order of metrics staying the same. If so, set this to true.
#   ## keeping the metrics ordered may be slightly slower.
#   ordered = false
#
#   [[processors.reverse_dns.lookup]]
#     ## get the ip from the field "source_ip", and put the result in the field "source_name"
#     field = "source_ip"
#     dest = "source_name"
#
#   [[processors.reverse_dns.lookup]]
#     ## get the ip from the tag "destination_ip", and put the result in the tag
#     ## "destination_name".
#     tag = "destination_ip"
#     dest = "destination_name"
#
#     ## If you would prefer destination_name to be a field instead, you can use a
#     ## processors.converter after this one, specifying the order attribute.


# # Add the S2 Cell ID as a tag based on latitude and longitude fields
# [[processors.s2geo]]
#   ## The name of the lat and lon fields containing WGS-84 latitude and
#   ## longitude in decimal degrees.
#   # lat_field = "lat"
#   # lon_field = "lon"
#
#   ## New tag to create
#   # tag_key = "s2_cell_id"
#
#   ## Cell level (see https://s2geometry.io/resources/s2cell_statistics.html)
#   # cell_level = 9


# # Scale values with a predefined range to a different output range.
# [[processors.scale]]
#     ## It is possible to define multiple different scaling that can be applied
#     ## do different sets of fields. Each scaling expects the following
#     ## arguments:
#     ##   - input_minimum: Minimum expected input value
#     ##   - input_maximum: Maximum expected input value
#     ##   - output_minimum: Minimum desired output value
#     ##   - output_maximum: Maximum desired output value
#     ## alternatively you can specify a scaling with factor and offset
#     ##   - factor: factor to scale the input value with
#     ##   - offset: additive offset for value after scaling
#     ##   - fields: a list of field names (or filters) to apply this scaling to
#
#     ## Example: Scaling with minimum and maximum values
#     # [processors.scale.scaling]
#     #    input_minimum = 0
#     #    input_maximum = 1
#     #    output_minimum = 0
#     #    output_maximum = 100
#     #    fields = ["temperature1", "temperature2"]
#
#     ## Example: Scaling with factor and offset
#     # [processors.scale.scaling]
#     #    factor = 10.0
#     #    offset = -5.0
#     #    fields = ["voltage*"]


# # Process metrics using a Starlark script
# [[processors.starlark]]
#   ## The Starlark source can be set as a string in this configuration file, or
#   ## by referencing a file containing the script.  Only one source or script
#   ## should be set at once.
#
#   ## Source of the Starlark script.
#   source = '''
# def apply(metric):
#   return metric
# '''
#
#   ## File containing a Starlark script.
#   # script = "/usr/local/bin/myscript.star"
#
#   ## The constants of the Starlark script.
#   # [processors.starlark.constants]
#   #   max_size = 10
#   #   threshold = 0.75
#   #   default_name = "Julia"
#   #   debug_mode = true


# # Perform string processing on tags, fields, and measurements
# [[processors.strings]]
#   ## Convert a field value to lowercase and store in a new field
#   # [[processors.strings.lowercase]]
#   #   field = "uri_stem"
#   #   dest = "uri_stem_normalised"
#
#   ## Convert a tag value to uppercase
#   # [[processors.strings.uppercase]]
#   #   tag = "method"
#
#   ## Convert a field value to titlecase
#   # [[processors.strings.titlecase]]
#   #   field = "status"
#
#   ## Trim leading and trailing whitespace using the default cutset
#   # [[processors.strings.trim]]
#   #   field = "message"
#
#   ## Trim leading characters in cutset
#   # [[processors.strings.trim_left]]
#   #   field = "message"
#   #   cutset = "\t"
#
#   ## Trim trailing characters in cutset
#   # [[processors.strings.trim_right]]
#   #   field = "message"
#   #   cutset = "\r\n"
#
#   ## Trim the given prefix from the field
#   # [[processors.strings.trim_prefix]]
#   #   field = "my_value"
#   #   prefix = "my_"
#
#   ## Trim the given suffix from the field
#   # [[processors.strings.trim_suffix]]
#   #   field = "read_count"
#   #   suffix = "_count"
#
#   ## Replace all non-overlapping instances of old with new
#   # [[processors.strings.replace]]
#   #   measurement = "*"
#   #   old = ":"
#   #   new = "_"
#
#   ## Trims strings based on width
#   # [[processors.strings.left]]
#   #   field = "message"
#   #   width = 10
#
#   ## Decode a base64 encoded utf-8 string
#   # [[processors.strings.base64decode]]
#   #   field = "message"
#
#   ## Sanitize a string to ensure it is a valid utf-8 string
#   ## Each run of invalid UTF-8 byte sequences is replaced by the replacement string, which may be empty
#   # [[processors.strings.valid_utf8]]
#   #   field = "message"
#   #   replacement = ""


# # Restricts the number of tags that can pass through this filter and chooses which tags to preserve when over the limit.
# [[processors.tag_limit]]
#   ## Maximum number of tags to preserve
#   limit = 3
#
#   ## List of tags to preferentially preserve
#   keep = ["environment", "region"]


# # Uses a Go template to create a new tag
# [[processors.template]]
#   ## Go template used to create the tag name of the output. In order to
#   ## ease TOML escaping requirements, you should use single quotes around
#   ## the template string.
#   tag = "topic"
#
#   ## Go template used to create the tag value of the output. In order to
#   ## ease TOML escaping requirements, you should use single quotes around
#   ## the template string.
#   template = '{{ .Tag "hostname" }}.{{ .Tag "level" }}'


# # Print all metrics that pass through this filter.
# [[processors.topk]]
#   ## How many seconds between aggregations
#   # period = 10
#
#   ## How many top buckets to return per field
#   ## Every field specified to aggregate over will return k number of results.
#   ## For example, 1 field with k of 10 will return 10 buckets. While 2 fields
#   ## with k of 3 will return 6 buckets.
#   # k = 10
#
#   ## Over which tags should the aggregation be done. Globs can be specified, in
#   ## which case any tag matching the glob will aggregated over. If set to an
#   ## empty list is no aggregation over tags is done
#   # group_by = ['*']
#
#   ## The field(s) to aggregate
#   ## Each field defined is used to create an independent aggregation. Each
#   ## aggregation will return k buckets. If a metric does not have a defined
#   ## field the metric will be dropped from the aggregation. Considering using
#   ## the defaults processor plugin to ensure fields are set if required.
#   # fields = ["value"]
#
#   ## What aggregation function to use. Options: sum, mean, min, max
#   # aggregation = "mean"
#
#   ## Instead of the top k largest metrics, return the bottom k lowest metrics
#   # bottomk = false
#
#   ## The plugin assigns each metric a GroupBy tag generated from its name and
#   ## tags. If this setting is different than "" the plugin will add a
#   ## tag (which name will be the value of this setting) to each metric with
#   ## the value of the calculated GroupBy tag. Useful for debugging
#   # add_groupby_tag = ""
#
#   ## These settings provide a way to know the position of each metric in
#   ## the top k. The 'add_rank_field' setting allows to specify for which
#   ## fields the position is required. If the list is non empty, then a field
#   ## will be added to each and every metric for each string present in this
#   ## setting. This field will contain the ranking of the group that
#   ## the metric belonged to when aggregated over that field.
#   ## The name of the field will be set to the name of the aggregation field,
#   ## suffixed with the string '_topk_rank'
#   # add_rank_fields = []
#
#   ## These settings provide a way to know what values the plugin is generating
#   ## when aggregating metrics. The 'add_aggregate_field' setting allows to
#   ## specify for which fields the final aggregation value is required. If the
#   ## list is non empty, then a field will be added to each every metric for
#   ## each field present in this setting. This field will contain
#   ## the computed aggregation for the group that the metric belonged to when
#   ## aggregated over that field.
#   ## The name of the field will be set to the name of the aggregation field,
#   ## suffixed with the string '_topk_aggregate'
#   # add_aggregate_fields = []


# # Rotate multi field metric into several single field metrics
# [[processors.unpivot]]
#   ## Metric mode to pivot to
#   ## Set to "tag", metrics are pivoted as a tag and the metric is kept as
#   ## the original measurement name. Tag key name is set by tag_key value.
#   ## Set to "metric" creates a new metric named the field name. With this
#   ## option the tag_key is ignored. Be aware that this could lead to metric
#   ## name conflicts!
#   # use_fieldname_as = "tag"
#
#   ## Tag to use for the name.
#   # tag_key = "name"
#
#   ## Field to use for the name of the value.
#   # value_key = "value"


###############################################################################
#                            AGGREGATOR PLUGINS                               #
###############################################################################


# # Keep the aggregate basicstats of each metric passing through.
# [[aggregators.basicstats]]
#   ## The period on which to flush & clear the aggregator.
#   period = "30s"
#
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = false
#
#   ## Configures which basic stats to push as fields
#   # stats = ["count","diff","rate","min","max","mean","non_negative_diff","non_negative_rate","percent_change","stdev","s2","sum","interval"]


# # Calculates a derivative for every field.
# [[aggregators.derivative]]
#   ## The period in which to flush the aggregator.
#   period = "30s"
#   ##
#   ## Suffix to append for the resulting derivative field.
#   # suffix = "_rate"
#   ##
#   ## Field to use for the quotient when computing the derivative.
#   ## When using a field as the derivation parameter the name of that field will
#   ## be used for the resulting derivative, e.g. *fieldname_by_parameter*.
#   ## By default the timestamps of the metrics are used and the suffix is omitted.
#   # variable = ""
#   ##
#   ## Maximum number of roll-overs in case only one measurement is found during a period.
#   # max_roll_over = 10


# # Report the final metric of a series
# [[aggregators.final]]
#   ## The period on which to flush & clear the aggregator.
#   period = "30s"
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = false
#
#   ## The time that a series is not updated until considering it final.
#   series_timeout = "5m"


# # Configuration for aggregate histogram metrics
# [[aggregators.histogram]]
#   ## The period in which to flush the aggregator.
#   period = "30s"
#
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = false
#
#   ## If true, the histogram will be reset on flush instead
#   ## of accumulating the results.
#   reset = false
#
#   ## Whether bucket values should be accumulated. If set to false, "gt" tag will be added.
#   ## Defaults to true.
#   cumulative = true
#
#   ## Expiration interval for each histogram. The histogram will be expired if
#   ## there are no changes in any buckets for this time interval. 0 == no expiration.
#   # expiration_interval = "0m"
#
#   ## If true, aggregated histogram are pushed to output only if it was updated since
#   ## previous push. Defaults to false.
#   # push_only_on_update = false
#
#   ## Example config that aggregates all fields of the metric.
#   # [[aggregators.histogram.config]]
#   #   ## Right borders of buckets (with +Inf implicitly added).
#   #   buckets = [0.0, 15.6, 34.5, 49.1, 71.5, 80.5, 94.5, 100.0]
#   #   ## The name of metric.
#   #   measurement_name = "cpu"
#
#   ## Example config that aggregates only specific fields of the metric.
#   # [[aggregators.histogram.config]]
#   #   ## Right borders of buckets (with +Inf implicitly added).
#   #   buckets = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]
#   #   ## The name of metric.
#   #   measurement_name = "diskio"
#   #   ## The concrete fields of metric
#   #   fields = ["io_time", "read_time", "write_time"]


# # Merge metrics into multifield metrics by series key
# [[aggregators.merge]]
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = true


# # Keep the aggregate min/max of each metric passing through.
# [[aggregators.minmax]]
#   ## General Aggregator Arguments:
#   ## The period on which to flush & clear the aggregator.
#   period = "30s"
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = false


# # Keep the aggregate quantiles of each metric passing through.
# [[aggregators.quantile]]
#   ## General Aggregator Arguments:
#   ## The period on which to flush & clear the aggregator.
#   period = "30s"
#
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = false
#
#   ## Quantiles to output in the range [0,1]
#   # quantiles = [0.25, 0.5, 0.75]
#
#   ## Type of aggregation algorithm
#   ## Supported are:
#   ##  "t-digest" -- approximation using centroids, can cope with large number of samples
#   ##  "exact R7" -- exact computation also used by Excel or NumPy (Hyndman & Fan 1996 R7)
#   ##  "exact R8" -- exact computation (Hyndman & Fan 1996 R8)
#   ## NOTE: Do not use "exact" algorithms with large number of samples
#   ##       to not impair performance or memory consumption!
#   # algorithm = "t-digest"
#
#   ## Compression for approximation (t-digest). The value needs to be
#   ## greater or equal to 1.0. Smaller values will result in more
#   ## performance but less accuracy.
#   # compression = 100.0


# # Aggregate metrics using a Starlark script
# [[aggregators.starlark]]
#   ## The Starlark source can be set as a string in this configuration file, or
#   ## by referencing a file containing the script.  Only one source or script
#   ## should be set at once.
#   ##
#   ## Source of the Starlark script.
#   source = '''
# state = {}
#
# def add(metric):
#   state["last"] = metric
#
# def push():
#   return state.get("last")
#
# def reset():
#   state.clear()
# '''
#
#   ## File containing a Starlark script.
#   # script = "/usr/local/bin/myscript.star"
#
#   ## The constants of the Starlark script.
#   # [aggregators.starlark.constants]
#   #   max_size = 10
#   #   threshold = 0.75
#   #   default_name = "Julia"
#   #   debug_mode = true


# # Count the occurrence of values in fields.
# [[aggregators.valuecounter]]
#   ## General Aggregator Arguments:
#   ## The period on which to flush & clear the aggregator.
#   period = "30s"
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = false
#   ## The fields for which the values will be counted
#   fields = ["status"]


###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################


# Read metrics about cpu usage
[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states
  report_active = false
  ## If true and the info is available then add core_id and physical_id tags
  core_tags = false


# Read metrics about disk usage by mount point
[[inputs.disk]]
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]

  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]

  ## Ignore mount points by mount options.
  ## The 'mount' command reports options of all mounts in parathesis.
  ## Bind mounts can be ignored with the special 'bind' option.
  # ignore_mount_opts = []


# Read metrics about disk IO by device
[[inputs.diskio]]
  ## By default, telegraf will gather stats for all devices including
  ## disk partitions.
  ## Setting devices will restrict the stats to the specified devices.
  ## NOTE: Globbing expressions (e.g. asterix) are not supported for
  ##       disk synonyms like '/dev/disk/by-id'.
  # devices = ["sda", "sdb", "vd*", "/dev/disk/by-id/nvme-eui.00123deadc0de123"]
  ## Uncomment the following line if you need disk serial numbers.
  # skip_serial_number = false
  #
  ## On systems which support it, device metadata can be added in the form of
  ## tags.
  ## Currently only Linux is supported via udev properties. You can view
  ## available properties for a device by running:
  ## 'udevadm info -q property -n /dev/sda'
  ## Note: Most, but not all, udev properties can be accessed this way. Properties
  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.
  # device_tags = ["ID_FS_TYPE", "ID_FS_USAGE"]
  #
  ## Using the same metadata source as device_tags, you can also customize the
  ## name of the device via templates.
  ## The 'name_templates' parameter is a list of templates to try and apply to
  ## the device. The template may contain variables in the form of '$PROPERTY' or
  ## '${PROPERTY}'. The first template which does not contain any variables not
  ## present for the device is used as the device name tag.
  ## The typical use case is for LVM volumes, to get the VG/LV name instead of
  ## the near-meaningless DM-0 name.
  # name_templates = ["$ID_FS_LABEL","$DM_VG_NAME/$DM_LV_NAME"]


# Get kernel statistics from /proc/stat
# This plugin ONLY supports Linux
[[inputs.kernel]]
  # no configuration


# Read metrics about memory usage
[[inputs.mem]]
  # no configuration


# Get the number of processes and group them by status
# This plugin ONLY supports non-Windows
[[inputs.processes]]
  ## Use sudo to run ps command on *BSD systems. Linux systems will read
  ## /proc, so this does not apply there.
  use_sudo = false


# Read metrics about swap memory usage
[[inputs.swap]]
  # no configuration


# Read metrics about system load & uptime
[[inputs.system]]
  # no configuration



# # Pull Metric Statistics from Amazon CloudWatch
# [[inputs.cloudwatch]]
#   ## Amazon Region
#   region = "us-east-1"
#
#   ## Amazon Credentials
#   ## Credentials are loaded in the following order
#   ## 1) Web identity provider credentials via STS if role_arn and
#   ##    web_identity_token_file are specified
#   ## 2) Assumed credentials via STS if role_arn is specified
#   ## 3) explicit credentials from 'access_key' and 'secret_key'
#   ## 4) shared profile from 'profile'
#   ## 5) environment variables
#   ## 6) shared credentials file
#   ## 7) EC2 Instance Profile
#   # access_key = ""
#   # secret_key = ""
#   # token = ""
#   # role_arn = ""
#   # web_identity_token_file = ""
#   # role_session_name = ""
#   # profile = ""
#   # shared_credential_file = ""
#
#   ## If you are using CloudWatch cross-account observability, you can
#   ## set IncludeLinkedAccounts to true in a monitoring account
#   ## and collect metrics from the linked source accounts
#   # include_linked_accounts = false
#
#   ## Endpoint to make request against, the correct endpoint is automatically
#   ## determined and this option should only be set if you wish to override the
#   ## default.
#   ##   ex: endpoint_url = "http://localhost:8000"
#   # endpoint_url = ""
#
#   ## Set http_proxy
#   # use_system_proxy = false
#   # http_proxy_url = "http://localhost:8888"
#
#   ## The minimum period for Cloudwatch metrics is 1 minute (60s). However not
#   ## all metrics are made available to the 1 minute period. Some are collected
#   ## at 3 minute, 5 minute, or larger intervals.
#   ## See https://aws.amazon.com/cloudwatch/faqs/#monitoring.
#   ## Note that if a period is configured that is smaller than the minimum for a
#   ## particular metric, that metric will not be returned by the Cloudwatch API
#   ## and will not be collected by Telegraf.
#   #
#   ## Requested CloudWatch aggregation Period (required)
#   ## Must be a multiple of 60s.
#   period = "5m"
#
#   ## Collection Delay (required)
#   ## Must account for metrics availability via CloudWatch API
#   delay = "5m"
#
#   ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid
#   ## gaps or overlap in pulled data
#   interval = "5m"
#
#   ## Recommended if "delay" and "period" are both within 3 hours of request
#   ## time. Invalid values will be ignored. Recently Active feature will only
#   ## poll for CloudWatch ListMetrics values that occurred within the last 3h.
#   ## If enabled, it will reduce total API usage of the CloudWatch ListMetrics
#   ## API and require less memory to retain.
#   ## Do not enable if "period" or "delay" is longer than 3 hours, as it will
#   ## not return data more than 3 hours old.
#   ## See https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_ListMetrics.html
#   #recently_active = "PT3H"
#
#   ## Configure the TTL for the internal cache of metrics.
#   # cache_ttl = "1h"
#
#   ## Metric Statistic Namespaces (required)
#   namespaces = ["AWS/ELB"]
#
#   ## Maximum requests per second. Note that the global default AWS rate limit
#   ## is 50 reqs/sec, so if you define multiple namespaces, these should add up
#   ## to a maximum of 50.
#   ## See http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_limits.html
#   # ratelimit = 25
#
#   ## Timeout for http requests made by the cloudwatch client.
#   # timeout = "5s"
#
#   ## Batch Size
#   ## The size of each batch to send requests to Cloudwatch. 500 is the
#   ## suggested largest size. If a request gets to large (413 errors), consider
#   ## reducing this amount.
#   # batch_size = 500
#
#   ## Namespace-wide statistic filters. These allow fewer queries to be made to
#   ## cloudwatch.
#   # statistic_include = ["average", "sum", "minimum", "maximum", sample_count"]
#   # statistic_exclude = []
#
#   ## Metrics to Pull
#   ## Defaults to all Metrics in Namespace if nothing is provided
#   ## Refreshes Namespace available metrics every 1h
#   #[[inputs.cloudwatch.metrics]]
#   #  names = ["Latency", "RequestCount"]
#   #
#   #  ## Statistic filters for Metric.  These allow for retrieving specific
#   #  ## statistics for an individual metric.
#   #  # statistic_include = ["average", "sum", "minimum", "maximum", sample_count"]
#   #  # statistic_exclude = []
#   #
#   #  ## Dimension filters for Metric.
#   #  ## All dimensions defined for the metric names must be specified in order
#   #  ## to retrieve the metric statistics.
#   #  ## 'value' has wildcard / 'glob' matching support such as 'p-*'.
#   #  [[inputs.cloudwatch.metrics.dimensions]]
#   #    name = "LoadBalancerName"
#   #    value = "p-example"


# # Read metrics about docker containers
# [[inputs.docker]]
#   ## Docker Endpoint
#   ##   To use TCP, set endpoint = "tcp://[ip]:[port]"
#   ##   To use environment variables (ie, docker-machine), set endpoint = "ENV"
#   endpoint = "unix:///var/run/docker.sock"
#
#   ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)
#   ## Note: configure this in one of the manager nodes in a Swarm cluster.
#   ## configuring in multiple Swarm managers results in duplication of metrics.
#   gather_services = false
#
#   ## Only collect metrics for these containers. Values will be appended to
#   ## container_name_include.
#   ## Deprecated (1.4.0), use container_name_include
#   container_names = []
#
#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars
#   source_tag = false
#
#   ## Containers to include and exclude. Collect all if empty. Globs accepted.
#   container_name_include = []
#   container_name_exclude = []
#
#   ## Container states to include and exclude. Globs accepted.
#   ## When empty only containers in the "running" state will be captured.
#   ## example: container_state_include = ["created", "restarting", "running", "removing", "paused", "exited", "dead"]
#   ## example: container_state_exclude = ["created", "restarting", "running", "removing", "paused", "exited", "dead"]
#   # container_state_include = []
#   # container_state_exclude = []
#
#   ## Timeout for docker list, info, and stats commands
#   timeout = "5s"
#
#   ## Whether to report for each container per-device blkio (8:0, 8:1...),
#   ## network (eth0, eth1, ...) and cpu (cpu0, cpu1, ...) stats or not.
#   ## Usage of this setting is discouraged since it will be deprecated in favor of 'perdevice_include'.
#   ## Default value is 'true' for backwards compatibility, please set it to 'false' so that 'perdevice_include' setting
#   ## is honored.
#   perdevice = true
#
#   ## Specifies for which classes a per-device metric should be issued
#   ## Possible values are 'cpu' (cpu0, cpu1, ...), 'blkio' (8:0, 8:1, ...) and 'network' (eth0, eth1, ...)
#   ## Please note that this setting has no effect if 'perdevice' is set to 'true'
#   # perdevice_include = ["cpu"]
#
#   ## Whether to report for each container total blkio and network stats or not.
#   ## Usage of this setting is discouraged since it will be deprecated in favor of 'total_include'.
#   ## Default value is 'false' for backwards compatibility, please set it to 'true' so that 'total_include' setting
#   ## is honored.
#   total = false
#
#   ## Specifies for which classes a total metric should be issued. Total is an aggregated of the 'perdevice' values.
#   ## Possible values are 'cpu', 'blkio' and 'network'
#   ## Total 'cpu' is reported directly by Docker daemon, and 'network' and 'blkio' totals are aggregated by this plugin.
#   ## Please note that this setting has no effect if 'total' is set to 'false'
#   # total_include = ["cpu", "blkio", "network"]
#
#   ## docker labels to include and exclude as tags.  Globs accepted.
#   ## Note that an empty array for both will include all labels as tags
#   docker_label_include = []
#   docker_label_exclude = []
#
#   ## Which environment variables should we use as a tag
#   tag_env = ["JAVA_HOME", "HEAP_SIZE"]
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false



# # Count files in a directory
# [[inputs.filecount]]
#   ## Directories to gather stats about.
#   ## This accept standard unit glob matching rules, but with the addition of
#   ## ** as a "super asterisk". ie:
#   ##   /var/log/**    -> recursively find all directories in /var/log and count files in each directories
#   ##   /var/log/*/*   -> find all directories with a parent dir in /var/log and count files in each directories
#   ##   /var/log       -> count all files in /var/log and all of its subdirectories
#   directories = ["/var/cache/apt", "/tmp"]
#
#   ## Only count files that match the name pattern. Defaults to "*".
#   name = "*"
#
#   ## Count files in subdirectories. Defaults to true.
#   recursive = true
#
#   ## Only count regular files. Defaults to true.
#   regular_only = true
#
#   ## Follow all symlinks while walking the directory tree. Defaults to false.
#   follow_symlinks = false
#
#   ## Only count files that are at least this size. If size is
#   ## a negative number, only count files that are smaller than the
#   ## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...
#   ## Without quotes and units, interpreted as size in bytes.
#   size = "0B"
#
#   ## Only count files that have not been touched for at least this
#   ## duration. If mtime is negative, only count files that have been
#   ## touched in this duration. Defaults to "0s".
#   mtime = "0s"




# # Collect kernel snmp counters and network interface statistics
# [[inputs.nstat]]
#   ## file paths for proc files. If empty default paths will be used:
#   ##    /proc/net/netstat, /proc/net/snmp, /proc/net/snmp6
#   ## These can also be overridden with env variables, see README.
#   proc_net_netstat = "/proc/net/netstat"
#   proc_net_snmp = "/proc/net/snmp"
#   r5proc_net_snmp6 = "/proc/net/snmp6"
#   ## dump metrics with 0 values too
#   dump_zeros       = true

# # Ping given url(s) and return statistics
# [[inputs.ping]]
#   ## Hosts to send ping packets to.
#   urls = ["example.org"]
#
#   ## Method used for sending pings, can be either "exec" or "native".  When set
#   ## to "exec" the systems ping command will be executed.  When set to "native"
#   ## the plugin will send pings directly.
#   ##
#   ## While the default is "exec" for backwards compatibility, new deployments
#   ## are encouraged to use the "native" method for improved compatibility and
#   ## performance.
#   # method = "exec"
#
#   ## Number of ping packets to send per interval.  Corresponds to the "-c"
#   ## option of the ping command.
#   # count = 1
#
#   ## Time to wait between sending ping packets in seconds.  Operates like the
#   ## "-i" option of the ping command.
#   # ping_interval = 1.0
#
#   ## If set, the time to wait for a ping response in seconds.  Operates like
#   ## the "-W" option of the ping command.
#   # timeout = 1.0
#
#   ## If set, the total ping deadline, in seconds.  Operates like the -w option
#   ## of the ping command.
#   # deadline = 10
#
#   ## Interface or source address to send ping from.  Operates like the -I or -S
#   ## option of the ping command.
#   # interface = ""
#
#   ## Percentiles to calculate. This only works with the native method.
#   # percentiles = [50, 95, 99]
#
#   ## Specify the ping executable binary.
#   # binary = "ping"
#
#   ## Arguments for ping command. When arguments is not empty, the command from
#   ## the binary option will be used and other options (ping_interval, timeout,
#   ## etc) will be ignored.
#   # arguments = ["-c", "3"]
#
#   ## Use only IPv6 addresses when resolving a hostname.
#   # ipv6 = false
#
#   ## Number of data bytes to be sent. Corresponds to the "-s"
#   ## option of the ping command. This only works with the native method.
#   # size = 56

# # Monitor process cpu and memory usage
# [[inputs.procstat]]
#   ## PID file to monitor process
#   pid_file = "/var/run/nginx.pid"
#   ## executable name (ie, pgrep <exe>)
#   # exe = "nginx"
#   ## pattern as argument for pgrep (ie, pgrep -f <pattern>)
#   # pattern = "nginx"
#   ## user as argument for pgrep (ie, pgrep -u <user>)
#   # user = "nginx"
#   ## Systemd unit name, supports globs when include_systemd_children is set to true
#   # systemd_unit = "nginx.service"
#   # include_systemd_children = false
#   ## CGroup name or path, supports globs
#   # cgroup = "systemd/system.slice/nginx.service"
#
#   ## Windows service name
#   # win_service = ""
#
#   ## override for process_name
#   ## This is optional; default is sourced from /proc/<pid>/status
#   # process_name = "bar"
#
#   ## Field name prefix
#   # prefix = ""
#
#   ## When true add the full cmdline as a tag.
#   # cmdline_tag = false
#
#   ## Mode to use when calculating CPU usage. Can be one of 'solaris' or 'irix'.
#   # mode = "irix"
#
#   ## Add the PID as a tag instead of as a field.  When collecting multiple
#   ## processes with otherwise matching tags this setting should be enabled to
#   ## ensure each process has a unique identity.
#   ##
#   ## Enabling this option may result in a large number of series, especially
#   ## when processes have a short lifetime.
#   # pid_tag = false
#
#   ## Method to use when finding process IDs.  Can be one of 'pgrep', or
#   ## 'native'.  The pgrep finder calls the pgrep executable in the PATH while
#   ## the native finder performs the search directly in a manor dependent on the
#   ## platform.  Default is 'pgrep'
#   # pid_finder = "pgrep"



###############################################################################
#                            SERVICE INPUT PLUGINS                            #
###############################################################################

# # Cisco model-driven telemetry (MDT) input plugin for IOS XR, IOS XE and NX-OS platforms
# [[inputs.cisco_telemetry_mdt]]
#  ## Telemetry transport can be "tcp" or "grpc".  TLS is only supported when
#  ## using the grpc transport.
#  transport = "grpc"
#
#  ## Address and port to host telemetry listener
#  service_address = ":57000"
#
#  ## Grpc Maximum Message Size, default is 4MB, increase the size. This is
#  ## stored as a uint32, and limited to 4294967295.
#  max_msg_size = 4000000
#
#  ## Enable TLS; grpc transport only.
#  # tls_cert = "/etc/telegraf/cert.pem"
#  # tls_key = "/etc/telegraf/key.pem"
#
#  ## Enable TLS client authentication and define allowed CA certificates; grpc
#  ##  transport only.
#  # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#  ## Define (for certain nested telemetry measurements with embedded tags) which fields are tags
#  # embedded_tags = ["Cisco-IOS-XR-qos-ma-oper:qos/interface-table/interface/input/service-policy-names/service-policy-instance/statistics/class-stats/class-name"]
#
#   ## Include the delete field in every telemetry message.
#   # include_delete_field = false
#
#  ## Define aliases to map telemetry encoding paths to simple measurement names
#  [inputs.cisco_telemetry_mdt.aliases]
#    ifstats = "ietf-interfaces:interfaces-state/interface/statistics"
#  ## Define Property Xformation, please refer README and https://pubhub.devnetcloud.com/media/dme-docs-9-3-3/docs/appendix/ for Model details.
#  [inputs.cisco_telemetry_mdt.dmes]
# #    Global Property Xformation.
# #    prop1 = "uint64 to int"
# #    prop2 = "uint64 to string"
# #    prop3 = "string to uint64"
# #    prop4 = "string to int64"
# #    prop5 = "string to float64"
# #    auto-prop-xfrom = "auto-float-xfrom" #Xform any property which is string, and has float number to type float64
# #    Per Path property xformation, Name is telemetry configuration under sensor-group, path configuration "WORD         Distinguished Name"
# #    Per Path configuration is better as it avoid property collision issue of types.
# #    dnpath = '{"Name": "show ip route summary","prop": [{"Key": "routes","Value": "string"}, {"Key": "best-paths","Value": "string"}]}'
# #    dnpath2 = '{"Name": "show processes cpu","prop": [{"Key": "kernel_percent","Value": "float"}, {"Key": "idle_percent","Value": "float"}, {"Key": "process","Value": "string"}, {"Key": "user_percent","Value": "float"}, {"Key": "onesec","Value": "float"}]}'
# #    dnpath3 = '{"Name": "show processes memory physical","prop": [{"Key": "processname","Value": "string"}]}'
#
#  ## Additional GRPC connection settings.
#  [inputs.cisco_telemetry_mdt.grpc_enforcement_policy]
#   ## GRPC permit keepalives without calls, set to true if your clients are
#   ## sending pings without calls in-flight. This can sometimes happen on IOS-XE
#   ## devices where the GRPC connection is left open but subscriptions have been
#   ## removed, and adding subsequent subscriptions does not keep a stable session.
#   # permit_keepalive_without_calls = false
#
#   ## GRPC minimum timeout between successive pings, decreasing this value may
#   ## help if this plugin is closing connections with ENHANCE_YOUR_CALM (too_many_pings).
#   # keepalive_minimum_time = "5m"


# # Read metrics from one or many ClickHouse servers
# [[inputs.clickhouse]]
#   ## Username for authorization on ClickHouse server
#   username = "default"
#
#   ## Password for authorization on ClickHouse server
#   # password = ""
#
#   ## HTTP(s) timeout while getting metrics values
#   ## The timeout includes connection time, any redirects, and reading the
#   ## response body.
#   # timeout = 5s
#
#   ## List of servers for metrics scraping
#   ## metrics scrape via HTTP(s) clickhouse interface
#   ## https://clickhouse.tech/docs/en/interfaces/http/
#   servers = ["http://127.0.0.1:8123"]
#
#   ## If "auto_discovery"" is "true" plugin tries to connect to all servers
#   ## available in the cluster with using same "user:password" described in
#   ## "user" and "password" parameters and get this server hostname list from
#   ## "system.clusters" table. See
#   ## - https://clickhouse.tech/docs/en/operations/system_tables/#system-clusters
#   ## - https://clickhouse.tech/docs/en/operations/server_settings/settings/#server_settings_remote_servers
#   ## - https://clickhouse.tech/docs/en/operations/table_engines/distributed/
#   ## - https://clickhouse.tech/docs/en/operations/table_engines/replication/#creating-replicated-tables
#   # auto_discovery = true
#
#   ## Filter cluster names in "system.clusters" when "auto_discovery" is "true"
#   ## when this filter present then "WHERE cluster IN (...)" filter will apply
#   ## please use only full cluster names here, regexp and glob filters is not
#   ## allowed for "/etc/clickhouse-server/config.d/remote.xml"
#   ## <yandex>
#   ##  <remote_servers>
#   ##    <my-own-cluster>
#   ##        <shard>
#   ##          <replica><host>clickhouse-ru-1.local</host><port>9000</port></replica>
#   ##          <replica><host>clickhouse-ru-2.local</host><port>9000</port></replica>
#   ##        </shard>
#   ##        <shard>
#   ##          <replica><host>clickhouse-eu-1.local</host><port>9000</port></replica>
#   ##          <replica><host>clickhouse-eu-2.local</host><port>9000</port></replica>
#   ##        </shard>
#   ##    </my-onw-cluster>
#   ##  </remote_servers>
#   ##
#   ## </yandex>
#   ##
#   ## example: cluster_include = ["my-own-cluster"]
#   # cluster_include = []
#
#   ## Filter cluster names in "system.clusters" when "auto_discovery" is
#   ## "true" when this filter present then "WHERE cluster NOT IN (...)"
#   ## filter will apply
#   ##    example: cluster_exclude = ["my-internal-not-discovered-cluster"]
#   # cluster_exclude = []
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false


# # Read metrics from Google PubSub
# [[inputs.cloud_pubsub]]
#   ## Required. Name of Google Cloud Platform (GCP) Project that owns
#   ## the given PubSub subscription.
#   project = "my-project"
#
#   ## Required. Name of PubSub subscription to ingest metrics from.
#   subscription = "my-subscription"
#
#   ## Required. Data format to consume.
#   ## Each data format has its own unique set of configuration options.
#   ## Read more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"
#
#   ## Optional. Filepath for GCP credentials JSON file to authorize calls to
#   ## PubSub APIs. If not set explicitly, Telegraf will attempt to use
#   ## Application Default Credentials, which is preferred.
#   # credentials_file = "path/to/my/creds.json"
#
#   ## Optional. Number of seconds to wait before attempting to restart the
#   ## PubSub subscription receiver after an unexpected error.
#   ## If the streaming pull for a PubSub Subscription fails (receiver),
#   ## the agent attempts to restart receiving messages after this many seconds.
#   # retry_delay_seconds = 5
#
#   ## Optional. Maximum byte length of a message to consume.
#   ## Larger messages are dropped with an error. If less than 0 or unspecified,
#   ## treated as no limit.
#   # max_message_len = 1000000
#
#   ## Max undelivered messages
#   ## This plugin uses tracking metrics, which ensure messages are read to
#   ## outputs before acknowledging them to the original broker to ensure data
#   ## is not lost. This option sets the maximum messages to read from the
#   ## broker that have not been written by an output.
#   ##
#   ## This value needs to be picked with awareness of the agent's
#   ## metric_batch_size value as well. Setting max undelivered messages too high
#   ## can result in a constant stream of data batches to the output. While
#   ## setting it too low may never flush the broker's messages.
#   # max_undelivered_messages = 1000
#
#   ## The following are optional Subscription ReceiveSettings in PubSub.
#   ## Read more about these values:
#   ## https://godoc.org/cloud.google.com/go/pubsub#ReceiveSettings
#
#   ## Optional. Maximum number of seconds for which a PubSub subscription
#   ## should auto-extend the PubSub ACK deadline for each message. If less than
#   ## 0, auto-extension is disabled.
#   # max_extension = 0
#
#   ## Optional. Maximum number of unprocessed messages in PubSub
#   ## (unacknowledged but not yet expired in PubSub).
#   ## A value of 0 is treated as the default PubSub value.
#   ## Negative values will be treated as unlimited.
#   # max_outstanding_messages = 0
#
#   ## Optional. Maximum size in bytes of unprocessed messages in PubSub
#   ## (unacknowledged but not yet expired in PubSub).
#   ## A value of 0 is treated as the default PubSub value.
#   ## Negative values will be treated as unlimited.
#   # max_outstanding_bytes = 0
#
#   ## Optional. Max number of goroutines a PubSub Subscription receiver can spawn
#   ## to pull messages from PubSub concurrently. This limit applies to each
#   ## subscription separately and is treated as the PubSub default if less than
#   ## 1. Note this setting does not limit the number of messages that can be
#   ## processed concurrently (use "max_outstanding_messages" instead).
#   # max_receiver_go_routines = 0
#
#   ## Optional. If true, Telegraf will attempt to base64 decode the
#   ## PubSub message data before parsing. Many GCP services that
#   ## output JSON to Google PubSub base64-encode the JSON payload.
#   # base64_data = false
#
#   ## Content encoding for message payloads, can be set to "gzip" or
#   ## "identity" to apply no encoding.
#   # content_encoding = "identity"
#
#   ## If content encoding is not "identity", sets the maximum allowed size,
#   ## in bytes, for a message payload when it's decompressed. Can be increased
#   ## for larger payloads or reduced to protect against decompression bombs.
#   ## Acceptable units are B, KiB, KB, MiB, MB...
#   # max_decompression_size = "500MB"



# # AWS Metric Streams listener
# [[inputs.cloudwatch_metric_streams]]
#   ## Address and port to host HTTP listener on
#   service_address = ":443"
#
#   ## Paths to listen to.
#   # paths = ["/telegraf"]
#
#   ## maximum duration before timing out read of the request
#   # read_timeout = "10s"
#
#   ## maximum duration before timing out write of the response
#   # write_timeout = "10s"
#
#   ## Maximum allowed http request body size in bytes.
#   ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)
#   # max_body_size = "500MB"
#
#   ## Optional access key for Firehose security.
#   # access_key = "test-key"
#
#   ## An optional flag to keep Metric Streams metrics compatible with
#   ## CloudWatch's API naming
#   # api_compatability = false
#
#   ## Set one or more allowed client CA certificate file names to
#   ## enable mutually authenticated TLS connections
#   # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#   ## Add service certificate and key
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"


# # A ctrlX Data Layer server sent event input plugin
# [[inputs.ctrlx_datalayer]]
#    ## Hostname or IP address of the ctrlX CORE Data Layer server
#    ##  example: server = "localhost"        # Telegraf is running directly on the device
#    ##           server = "192.168.1.1"      # Connect to ctrlX CORE remote via IP
#    ##           server = "host.example.com" # Connect to ctrlX CORE remote via hostname
#    ##           server = "10.0.2.2:8443"    # Connect to ctrlX CORE Virtual from development environment
#    server = "localhost"
#
#    ## Authentication credentials
#    username = "boschrexroth"
#    password = "boschrexroth"
#
#    ## Use TLS but skip chain & host verification
#    # insecure_skip_verify = false
#
#    ## Timeout for HTTP requests. (default: "10s")
#    # timeout = "10s"
#
#
#    ## Create a ctrlX Data Layer subscription.
#    ## It is possible to define multiple subscriptions per host. Each subscription can have its own
#    ## sampling properties and a list of nodes to subscribe to.
#    ## All subscriptions share the same credentials.
#    [[inputs.ctrlx_datalayer.subscription]]
#       ## The name of the measurement. (default: "ctrlx")
#       measurement = "memory"
#
#       ## Configure the ctrlX Data Layer nodes which should be subscribed.
#       ## address - node address in ctrlX Data Layer (mandatory)
#       ## name    - field name to use in the output (optional, default: base name of address)
#       ## tags    - extra node tags to be added to the output metric (optional)
#       ## Note:
#       ## Use either the inline notation or the bracketed notation, not both.
#       ## The tags property is only supported in bracketed notation due to toml parser restrictions
#       ## Examples:
#       ## Inline notation
#       nodes=[
#          {name="available", address="framework/metrics/system/memavailable-mb"},
#          {name="used", address="framework/metrics/system/memused-mb"},
#       ]
#       ## Bracketed notation
#       # [[inputs.ctrlx_datalayer.subscription.nodes]]
#       #    name   ="available"
#       #    address="framework/metrics/system/memavailable-mb"
#       #    ## Define extra tags related to node to be added to the output metric (optional)
#       #    [inputs.ctrlx_datalayer.subscription.nodes.tags]
#       #       node_tag1="node_tag1"
#       #       node_tag2="node_tag2"
#       # [[inputs.ctrlx_datalayer.subscription.nodes]]
#       #    name   ="used"
#       #    address="framework/metrics/system/memused-mb"
#
#       ## The switch "output_json_string" enables output of the measurement as json.
#       ## That way it can be used in in a subsequent processor plugin, e.g. "Starlark Processor Plugin".
#       # output_json_string = false
#
#       ## Define extra tags related to subscription to be added to the output metric (optional)
#       # [inputs.ctrlx_datalayer.subscription.tags]
#       #    subscription_tag1 = "subscription_tag1"
#       #    subscription_tag2 = "subscription_tag2"
#
#       ## The interval in which messages shall be sent by the ctrlX Data Layer to this plugin. (default: 1s)
#       ## Higher values reduce load on network by queuing samples on server side and sending as a single TCP packet.
#       # publish_interval = "1s"
#
#       ## The interval a "keepalive" message is sent if no change of data occurs. (default: 60s)
#       ## Only used internally to detect broken network connections.
#       # keep_alive_interval = "60s"
#
#       ## The interval an "error" message is sent if an error was received from a node. (default: 10s)
#       ## Higher values reduce load on output target and network in case of errors by limiting frequency of error messages.
#       # error_interval = "10s"
#
#       ## The interval that defines the fastest rate at which the node values should be sampled and values captured. (default: 1s)
#       ## The sampling frequency should be adjusted to the dynamics of the signal to be sampled.
#       ## Higher sampling frequence increases load on ctrlX Data Layer.
#       ## The sampling frequency can be higher, than the publish interval. Captured samples are put in a queue and sent in publish interval.
#       ## Note: The minimum sampling interval can be overruled by a global setting in the ctrlX Data Layer configuration ('datalayer/subscriptions/settings').
#       # sampling_interval = "1s"
#
#       ## The requested size of the node value queue. (default: 10)
#       ## Relevant if more values are captured than can be sent.
#       # queue_size = 10
#
#       ## The behaviour of the queue if it is full. (default: "DiscardOldest")
#       ## Possible values:
#       ## - "DiscardOldest"
#       ##   The oldest value gets deleted from the queue when it is full.
#       ## - "DiscardNewest"
#       ##   The newest value gets deleted from the queue when it is full.
#       # queue_behaviour = "DiscardOldest"
#
#       ## The filter when a new value will be sampled. (default: 0.0)
#       ## Calculation rule: If (abs(lastCapturedValue - newValue) > dead_band_value) capture(newValue).
#       # dead_band_value = 0.0
#
#       ## The conditions on which a sample should be captured and thus will be sent as a message. (default: "StatusValue")
#       ## Possible values:
#       ## - "Status"
#       ##   Capture the value only, when the state of the node changes from or to error state. Value changes are ignored.
#       ## - "StatusValue"
#       ##   Capture when the value changes or the node changes from or to error state.
#       ##   See also 'dead_band_value' for what is considered as a value change.
#       ## - "StatusValueTimestamp":
#       ##   Capture even if the value is the same, but the timestamp of the value is newer.
#       ##   Note: This might lead to high load on the network because every sample will be sent as a message
#       ##   even if the value of the node did not change.
#       # value_change = "StatusValue"
#



# # Read logging output from the Docker engine
 [[inputs.docker_log]]
#   ## Docker Endpoint
#   ##   To use TCP, set endpoint = "tcp://[ip]:[port]"
#   ##   To use environment variables (ie, docker-machine), set endpoint = "ENV"
    endpoint = "unix:///var/run/docker.sock"
#
#   ## When true, container logs are read from the beginning; otherwise reading
#   ## begins at the end of the log. If state-persistence is enabled for Telegraf,
#   ## the reading continues at the last previously processed timestamp.
    from_beginning = true
#
#   ## Timeout for Docker API calls.
    timeout = "5s"
#
#   ## Containers to include and exclude. Globs accepted.
#   ## Note that an empty array for both will include all containers
#   # container_name_include = []
#   # container_name_exclude = []
#
#   ## Container states to include and exclude. Globs accepted.
#   ## When empty only containers in the "running" state will be captured.
#   # container_state_include = []
#   # container_state_exclude = []
#
#   ## docker labels to include and exclude as tags.  Globs accepted.
#   ## Note that an empty array for both will include all labels as tags
#   # docker_label_include = []
#   # docker_label_exclude = []
#
#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars
   source_tag = false
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false


# # Azure Event Hubs service input plugin
# [[inputs.eventhub_consumer]]
#   ## The default behavior is to create a new Event Hub client from environment variables.
#   ## This requires one of the following sets of environment variables to be set:
#   ##
#   ## 1) Expected Environment Variables:
#   ##    - "EVENTHUB_CONNECTION_STRING"
#   ##
#   ## 2) Expected Environment Variables:
#   ##    - "EVENTHUB_NAMESPACE"
#   ##    - "EVENTHUB_NAME"
#   ##    - "EVENTHUB_KEY_NAME"
#   ##    - "EVENTHUB_KEY_VALUE"
#
#   ## 3) Expected Environment Variables:
#   ##    - "EVENTHUB_NAMESPACE"
#   ##    - "EVENTHUB_NAME"
#   ##    - "AZURE_TENANT_ID"
#   ##    - "AZURE_CLIENT_ID"
#   ##    - "AZURE_CLIENT_SECRET"
#
#   ## Uncommenting the option below will create an Event Hub client based solely on the connection string.
#   ## This can either be the associated environment variable or hard coded directly.
#   ## If this option is uncommented, environment variables will be ignored.
#   ## Connection string should contain EventHubName (EntityPath)
#   # connection_string = ""
#
#   ## Set persistence directory to a valid folder to use a file persister instead of an in-memory persister
#   # persistence_dir = ""
#
#   ## Change the default consumer group
#   # consumer_group = ""
#
#   ## By default the event hub receives all messages present on the broker, alternative modes can be set below.
#   ## The timestamp should be in https://github.com/toml-lang/toml#offset-date-time format (RFC 3339).
#   ## The 3 options below only apply if no valid persister is read from memory or file (e.g. first run).
#   # from_timestamp =
#   # latest = true
#
#   ## Set a custom prefetch count for the receiver(s)
#   # prefetch_count = 1000
#
#   ## Add an epoch to the receiver(s)
#   # epoch = 0
#
#   ## Change to set a custom user agent, "telegraf" is used by default
#   # user_agent = "telegraf"
#
#   ## To consume from a specific partition, set the partition_ids option.
#   ## An empty array will result in receiving from all partitions.
#   # partition_ids = ["0","1"]
#
#   ## Max undelivered messages
#   ## This plugin uses tracking metrics, which ensure messages are read to
#   ## outputs before acknowledging them to the original broker to ensure data
#   ## is not lost. This option sets the maximum messages to read from the
#   ## broker that have not been written by an output.
#   ##
#   ## This value needs to be picked with awareness of the agent's
#   ## metric_batch_size value as well. Setting max undelivered messages too high
#   ## can result in a constant stream of data batches to the output. While
#   ## setting it too low may never flush the broker's messages.
#   # max_undelivered_messages = 1000
#
#   ## Set either option below to true to use a system property as timestamp.
#   ## You have the choice between EnqueuedTime and IoTHubEnqueuedTime.
#   ## It is recommended to use this setting when the data itself has no timestamp.
#   # enqueued_time_as_ts = true
#   # iot_hub_enqueued_time_as_ts = true
#
#   ## Tags or fields to create from keys present in the application property bag.
#   ## These could for example be set by message enrichments in Azure IoT Hub.
#   # application_property_tags = []
#   # application_property_fields = []
#
#   ## Tag or field name to use for metadata
#   ## By default all metadata is disabled
#   # sequence_number_field = "SequenceNumber"
#   # enqueued_time_field = "EnqueuedTime"
#   # offset_field = "Offset"
#   # partition_id_tag = "PartitionID"
#   # partition_key_tag = "PartitionKey"
#   # iot_hub_device_connection_id_tag = "IoTHubDeviceConnectionID"
#   # iot_hub_auth_generation_id_tag = "IoTHubAuthGenerationID"
#   # iot_hub_connection_auth_method_tag = "IoTHubConnectionAuthMethod"
#   # iot_hub_connection_module_id_tag = "IoTHubConnectionModuleID"
#   # iot_hub_enqueued_time_field = "IoTHubEnqueuedTime"
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"


# # Run executable as long-running input plugin
# [[inputs.execd]]
#   ## One program to run as daemon.
#   ## NOTE: process and each argument should each be their own string
#   command = ["telegraf-smartctl", "-d", "/dev/sda"]
#
#   ## Environment variables
#   ## Array of "key=value" pairs to pass as environment variables
#   ## e.g. "KEY=value", "USERNAME=John Doe",
#   ## "LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs"
#   # environment = []
#
#   ## Define how the process is signaled on each collection interval.
#   ## Valid values are:
#   ##   "none"    : Do not signal anything. (Recommended for service inputs)
#   ##               The process must output metrics by itself.
#   ##   "STDIN"   : Send a newline on STDIN. (Recommended for gather inputs)
#   ##   "SIGHUP"  : Send a HUP signal. Not available on Windows. (not recommended)
#   ##   "SIGUSR1" : Send a USR1 signal. Not available on Windows.
#   ##   "SIGUSR2" : Send a USR2 signal. Not available on Windows.
#   signal = "none"
#
#   ## Delay before the process is restarted after an unexpected termination
#   restart_delay = "10s"
#
#   ## Buffer size used to read from the command output stream
#   ## Optional parameter. Default is 64 Kib, minimum is 16 bytes
#   # buffer_size = "64Kib"
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"


# # gNMI telemetry input plugin
# [[inputs.gnmi]]
#   ## Address and port of the gNMI GRPC server
#   addresses = ["10.49.234.114:57777"]
#
#   ## define credentials
#   username = "cisco"
#   password = "cisco"
#
#   ## gNMI encoding requested (one of: "proto", "json", "json_ietf", "bytes")
#   # encoding = "proto"
#
#   ## redial in case of failures after
#   # redial = "10s"
#
#   ## gRPC Maximum Message Size
#   # max_msg_size = "4MB"
#
#   ## Enable to get the canonical path as field-name
#   # canonical_field_names = false
#
#   ## Remove leading slashes and dots in field-name
#   # trim_field_names = false
#
#   ## enable client-side TLS and define CA to authenticate the device
#   # enable_tls = false
#   # tls_ca = "/etc/telegraf/ca.pem"
#   ## Minimal TLS version to accept by the client
#   # tls_min_version = "TLS12"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = true
#
#   ## define client-side TLS certificate & key to authenticate to the device
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#
#   ## gNMI subscription prefix (optional, can usually be left empty)
#   ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths
#   # origin = ""
#   # prefix = ""
#   # target = ""
#
#   ## Vendor specific options
#   ## This defines what vendor specific options to load.
#   ## * Juniper Header Extension (juniper_header): some sensors are directly managed by
#   ##   Linecard, which adds the Juniper GNMI Header Extension. Enabling this
#   ##   allows the decoding of the Extension header if present. Currently this knob
#   ##   adds component, component_id & sub_component_id as additionnal tags
#   # vendor_specific = []
#
#   ## Define additional aliases to map encoding paths to measurement names
#   # [inputs.gnmi.aliases]
#   #   ifcounters = "openconfig:/interfaces/interface/state/counters"
#
#   [[inputs.gnmi.subscription]]
#     ## Name of the measurement that will be emitted
#     name = "ifcounters"
#
#     ## Origin and path of the subscription
#     ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths
#     ##
#     ## origin usually refers to a (YANG) data model implemented by the device
#     ## and path to a specific substructure inside it that should be subscribed
#     ## to (similar to an XPath). YANG models can be found e.g. here:
#     ## https://github.com/YangModels/yang/tree/master/vendor/cisco/xr
#     origin = "openconfig-interfaces"
#     path = "/interfaces/interface/state/counters"
#
#     ## Subscription mode ("target_defined", "sample", "on_change") and interval
#     subscription_mode = "sample"
#     sample_interval = "10s"
#
#     ## Suppress redundant transmissions when measured values are unchanged
#     # suppress_redundant = false
#
#     ## If suppression is enabled, send updates at least every X seconds anyway
#     # heartbeat_interval = "60s"
#
#   ## Tag subscriptions are applied as tags to other subscriptions.
#   # [[inputs.gnmi.tag_subscription]]
#   #  ## When applying this value as a tag to other metrics, use this tag name
#   #  name = "descr"
#   #
#   #  ## All other subscription fields are as normal
#   #  origin = "openconfig-interfaces"
#   #  path = "/interfaces/interface/state"
#   #  subscription_mode = "on_change"
#   #
#   #  ## Match strategy to use for the tag.
#   #  ## Tags are only applied for metrics of the same address. The following
#   #  ## settings are valid:
#   #  ##   unconditional -- always match
#   #  ##   name          -- match by the "name" key
#   #  ##                    This resembles the previsou 'tag-only' behavior.
#   #  ##   elements      -- match by the keys in the path filtered by the path
#   #  ##                    parts specified `elements` below
#   #  ## By default, 'elements' is used if the 'elements' option is provided,
#   #  ## otherwise match by 'name'.
#   #  # match = ""
#   #
#   #  ## For the 'elements' match strategy, at least one path-element name must
#   #  ## be supplied containing at least one key to match on. Multiple path
#   #  ## elements can be specified in any order. All given keys must be equal
#   #  ## for a match.
#   #  # elements = ["description", "interface"]


#  ## DEPRECATED: The "http_listener" plugin is deprecated in version 1.9.0, has been renamed to 'influxdb_listener', use 'inputs.influxdb_listener' or 'inputs.http_listener_v2' instead.
# # Accept metrics over InfluxDB 1.x HTTP API
# [[inputs.influxdb_listener]]
#   ## Address and port to host HTTP listener on
#   service_address = ":8186"
#
#   ## maximum duration before timing out read of the request
#   read_timeout = "10s"
#   ## maximum duration before timing out write of the response
#   write_timeout = "10s"
#
#   ## Maximum allowed HTTP request body size in bytes.
#   ## 0 means to use the default of 32MiB.
#   max_body_size = 0
#
#   ## Maximum line size allowed to be sent in bytes.
#   ##   deprecated in 1.14; parser now handles lines of unlimited length and option is ignored
#   # max_line_size = 0
#
#   ## Set one or more allowed client CA certificate file names to
#   ## enable mutually authenticated TLS connections
#   tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#   ## Add service certificate and key
#   tls_cert = "/etc/telegraf/cert.pem"
#   tls_key = "/etc/telegraf/key.pem"
#
#   ## Optional tag name used to store the database name.
#   ## If the write has a database in the query string then it will be kept in this tag name.
#   ## This tag can be used in downstream outputs.
#   ## The default value of nothing means it will be off and the database will not be recorded.
#   ## If you have a tag that is the same as the one specified below, and supply a database,
#   ## the tag will be overwritten with the database supplied.
#   # database_tag = ""
#
#   ## If set the retention policy specified in the write query will be added as
#   ## the value of this tag name.
#   # retention_policy_tag = ""
#
#   ## Optional username and password to accept for HTTP basic authentication.
#   ## You probably want to make sure you have TLS configured above for this.
#   # basic_username = "foobar"
#   # basic_password = "barfoo"
#
#   ## Influx line protocol parser
#   ## 'internal' is the default. 'upstream' is a newer parser that is faster
#   ## and more memory efficient.
#   # parser_type = "internal"


# # Generic HTTP write listener
# [[inputs.http_listener_v2]]
#   ## Address and port to host HTTP listener on
#   service_address = ":8080"
#
#   ## Paths to listen to.
#   # paths = ["/telegraf"]
#
#   ## Save path as http_listener_v2_path tag if set to true
#   # path_tag = false
#
#   ## HTTP methods to accept.
#   # methods = ["POST", "PUT"]
#
#   ## Optional HTTP headers
#   ## These headers are applied to the server that is listening for HTTP
#   ## requests and included in responses.
#   # http_headers = {"HTTP_HEADER" = "TAG_NAME"}
#
#   ## maximum duration before timing out read of the request
#   # read_timeout = "10s"
#   ## maximum duration before timing out write of the response
#   # write_timeout = "10s"
#
#   ## Maximum allowed http request body size in bytes.
#   ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)
#   # max_body_size = "500MB"
#
#   ## Part of the request to consume.  Available options are "body" and
#   ## "query".
#   # data_source = "body"
#
#   ## Set one or more allowed client CA certificate file names to
#   ## enable mutually authenticated TLS connections
#   # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#   ## Add service certificate and key
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#
#   ## Minimal TLS version accepted by the server
#   # tls_min_version = "TLS12"
#
#   ## Optional username and password to accept for HTTP basic authentication.
#   ## You probably want to make sure you have TLS configured above for this.
#   # basic_username = "foobar"
#   # basic_password = "barfoo"
#
#   ## Optional setting to map http headers into tags
#   ## If the http header is not present on the request, no corresponding tag will be added
#   ## If multiple instances of the http header are present, only the first value will be used
#   # http_header_tags = {"HTTP_HEADER" = "TAG_NAME"}
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"


# # Accept metrics over InfluxDB 1.x HTTP API
# [[inputs.influxdb_listener]]
#   ## Address and port to host HTTP listener on
#   service_address = ":8186"
#
#   ## maximum duration before timing out read of the request
#   read_timeout = "10s"
#   ## maximum duration before timing out write of the response
#   write_timeout = "10s"
#
#   ## Maximum allowed HTTP request body size in bytes.
#   ## 0 means to use the default of 32MiB.
#   max_body_size = 0
#
#   ## Maximum line size allowed to be sent in bytes.
#   ##   deprecated in 1.14; parser now handles lines of unlimited length and option is ignored
#   # max_line_size = 0
#
#   ## Set one or more allowed client CA certificate file names to
#   ## enable mutually authenticated TLS connections
#   tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#   ## Add service certificate and key
#   tls_cert = "/etc/telegraf/cert.pem"
#   tls_key = "/etc/telegraf/key.pem"
#
#   ## Optional tag name used to store the database name.
#   ## If the write has a database in the query string then it will be kept in this tag name.
#   ## This tag can be used in downstream outputs.
#   ## The default value of nothing means it will be off and the database will not be recorded.
#   ## If you have a tag that is the same as the one specified below, and supply a database,
#   ## the tag will be overwritten with the database supplied.
#   # database_tag = ""
#
#   ## If set the retention policy specified in the write query will be added as
#   ## the value of this tag name.
#   # retention_policy_tag = ""
#
#   ## Optional username and password to accept for HTTP basic authentication.
#   ## You probably want to make sure you have TLS configured above for this.
#   # basic_username = "foobar"
#   # basic_password = "barfoo"
#
#   ## Influx line protocol parser
#   ## 'internal' is the default. 'upstream' is a newer parser that is faster
#   ## and more memory efficient.
#   # parser_type = "internal"


# # Accept metrics over InfluxDB 2.x HTTP API
# [[inputs.influxdb_v2_listener]]
#   ## Address and port to host InfluxDB listener on
#   ## (Double check the port. Could be 9999 if using OSS Beta)
#   service_address = ":8086"
#
#   ## Maximum duration before timing out read of the request
#   # read_timeout = "10s"
#   ## Maximum duration before timing out write of the response
#   # write_timeout = "10s"
#
#   ## Maximum allowed HTTP request body size in bytes.
#   ## 0 means to use the default of 32MiB.
#   # max_body_size = "32MiB"
#
#   ## Optional tag to determine the bucket.
#   ## If the write has a bucket in the query string then it will be kept in this tag name.
#   ## This tag can be used in downstream outputs.
#   ## The default value of nothing means it will be off and the database will not be recorded.
#   # bucket_tag = ""
#
#   ## Set one or more allowed client CA certificate file names to
#   ## enable mutually authenticated TLS connections
#   # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#   ## Add service certificate and key
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#
#   ## Optional token to accept for HTTP authentication.
#   ## You probably want to make sure you have TLS configured above for this.
#   # token = "some-long-shared-secret-token"
#
#   ## Influx line protocol parser
#   ## 'internal' is the default. 'upstream' is a newer parser that is faster
#   ## and more memory efficient.
#   # parser_type = "internal"


# # Intel Performance Monitoring Unit plugin exposes Intel PMU metrics available through Linux Perf subsystem
# # This plugin ONLY supports Linux on amd64
# [[inputs.intel_pmu]]
#   ## List of filesystem locations of JSON files that contain PMU event definitions.
#   event_definitions = ["/var/cache/pmu/GenuineIntel-6-55-4-core.json", "/var/cache/pmu/GenuineIntel-6-55-4-uncore.json"]
#
#   ## List of core events measurement entities. There can be more than one core_events sections.
#   [[inputs.intel_pmu.core_events]]
#     ## List of events to be counted. Event names shall match names from event_definitions files.
#     ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.
#     ## If absent, all core events from provided event_definitions are counted skipping unresolvable ones.
#     events = ["INST_RETIRED.ANY", "CPU_CLK_UNHALTED.THREAD_ANY:config1=0x4043200000000k"]
#
#     ## Limits the counting of events to core numbers specified.
#     ## If absent, events are counted on all cores.
#     ## Single "0", multiple "0,1,2" and range "0-2" notation is supported for each array element.
#     ##   example: cores = ["0,2", "4", "12-16"]
#     cores = ["0"]
#
#     ## Indicator that plugin shall attempt to run core_events.events as a single perf group.
#     ## If absent or set to false, each event is counted individually. Defaults to false.
#     ## This limits the number of events that can be measured to a maximum of available hardware counters per core.
#     ## Could vary depending on type of event, use of fixed counters.
#     # perf_group = false
#
#     ## Optionally set a custom tag value that will be added to every measurement within this events group.
#     ## Can be applied to any group of events, unrelated to perf_group setting.
#     # events_tag = ""
#
#   ## List of uncore event measurement entities. There can be more than one uncore_events sections.
#   [[inputs.intel_pmu.uncore_events]]
#     ## List of events to be counted. Event names shall match names from event_definitions files.
#     ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.
#     ## If absent, all uncore events from provided event_definitions are counted skipping unresolvable ones.
#     events = ["UNC_CHA_CLOCKTICKS", "UNC_CHA_TOR_OCCUPANCY.IA_MISS"]
#
#     ## Limits the counting of events to specified sockets.
#     ## If absent, events are counted on all sockets.
#     ## Single "0", multiple "0,1" and range "0-1" notation is supported for each array element.
#     ##   example: sockets = ["0-2"]
#     sockets = ["0"]
#
#     ## Indicator that plugin shall provide an aggregated value for multiple units of same type distributed in an uncore.
#     ## If absent or set to false, events for each unit are exposed as separate metric. Defaults to false.
#     # aggregate_uncore_units = false
#
#     ## Optionally set a custom tag value that will be added to every measurement within this events group.
#     # events_tag = ""


# # Read Intel RDT metrics
# # This plugin ONLY supports non-Windows
# [[inputs.intel_rdt]]
#   ## Optionally set sampling interval to Nx100ms.
#   ## This value is propagated to pqos tool. Interval format is defined by pqos itself.
#   ## If not provided or provided 0, will be set to 10 = 10x100ms = 1s.
#   # sampling_interval = "10"
#
#   ## Optionally specify the path to pqos executable.
#   ## If not provided, auto discovery will be performed.
#   # pqos_path = "/usr/local/bin/pqos"
#
#   ## Optionally specify if IPC and LLC_Misses metrics shouldn't be propagated.
#   ## If not provided, default value is false.
#   # shortened_metrics = false
#
#   ## Specify the list of groups of CPU core(s) to be provided as pqos input.
#   ## Mandatory if processes aren't set and forbidden if processes are specified.
#   ## e.g. ["0-3", "4,5,6"] or ["1-3,4"]
#   # cores = ["0-3"]
#
#   ## Specify the list of processes for which Metrics will be collected.
#   ## Mandatory if cores aren't set and forbidden if cores are specified.
#   ## e.g. ["qemu", "pmd"]
#   # processes = ["process"]
#
#   ## Specify if the pqos process should be called with sudo.
#   ## Mandatory if the telegraf process does not run as root.
#   # use_sudo = false


# # Subscribe and receive OpenConfig Telemetry data using JTI
# [[inputs.jti_openconfig_telemetry]]
#   ## List of device addresses to collect telemetry from
#   servers = ["localhost:1883"]
#
#   ## Authentication details. Username and password are must if device expects
#   ## authentication. Client ID must be unique when connecting from multiple instances
#   ## of telegraf to the same device
#   username = "user"
#   password = "pass"
#   client_id = "telegraf"
#
#   ## Frequency to get data
#   sample_frequency = "1000ms"
#
#   ## Sensors to subscribe for
#   ## A identifier for each sensor can be provided in path by separating with space
#   ## Else sensor path will be used as identifier
#   ## When identifier is used, we can provide a list of space separated sensors.
#   ## A single subscription will be created with all these sensors and data will
#   ## be saved to measurement with this identifier name
#   sensors = [
#    "/interfaces/",
#    "collection /components/ /lldp",
#   ]
#
#   ## We allow specifying sensor group level reporting rate. To do this, specify the
#   ## reporting rate in Duration at the beginning of sensor paths / collection
#   ## name. For entries without reporting rate, we use configured sample frequency
#   sensors = [
#    "1000ms customReporting /interfaces /lldp",
#    "2000ms collection /components",
#    "/interfaces",
#   ]
#
#   ## Timestamp Source
#   ## Set to 'collection' for time of collection, and 'data' for using the time
#   ## provided by the _timestamp field.
#   # timestamp_source = "collection"
#
#   ## Optional TLS Config
#   # enable_tls = false
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Minimal TLS version to accept by the client
#   # tls_min_version = "TLS12"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Delay between retry attempts of failed RPC calls or streams. Defaults to 1000ms.
#   ## Failed streams/calls will not be retried if 0 is provided
#   retry_delay = "1000ms"
#
#   ## To treat all string values as tags, set this to true
#   str_as_tags = false


# # Read metrics from Kafka topics
# [[inputs.kafka_consumer]]
#   ## Kafka brokers.
#   brokers = ["localhost:9092"]
#
#   ## Topics to consume.
#   topics = ["telegraf"]
#
#   ## Topic regular expressions to consume.  Matches will be added to topics.
#   ## Example: topic_regexps = [ "*test", "metric[0-9A-z]*" ]
#   # topic_regexps = [ ]
#
#   ## When set this tag will be added to all metrics with the topic as the value.
#   # topic_tag = ""
#
#   ## Optional Client id
#   # client_id = "Telegraf"
#
#   ## Set the minimal supported Kafka version.  Setting this enables the use of new
#   ## Kafka features and APIs.  Must be 0.10.2.0 or greater.
#   ##   ex: version = "1.1.0"
#   # version = ""
#
#   ## Optional TLS Config
#   # enable_tls = false
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Period between keep alive probes.
#   ## Defaults to the OS configuration if not specified or zero.
#   # keep_alive_period = "15s"
#
#   ## SASL authentication credentials.  These settings should typically be used
#   ## with TLS encryption enabled
#   # sasl_username = "kafka"
#   # sasl_password = "secret"
#
#   ## Optional SASL:
#   ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI
#   ## (defaults to PLAIN)
#   # sasl_mechanism = ""
#
#   ## used if sasl_mechanism is GSSAPI
#   # sasl_gssapi_service_name = ""
#   # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH
#   # sasl_gssapi_auth_type = "KRB5_USER_AUTH"
#   # sasl_gssapi_kerberos_config_path = "/"
#   # sasl_gssapi_realm = "realm"
#   # sasl_gssapi_key_tab_path = ""
#   # sasl_gssapi_disable_pafxfast = false
#
#   ## used if sasl_mechanism is OAUTHBEARER
#   # sasl_access_token = ""
#
#   ## SASL protocol version.  When connecting to Azure EventHub set to 0.
#   # sasl_version = 1
#
#   # Disable Kafka metadata full fetch
#   # metadata_full = false
#
#   ## Name of the consumer group.
#   # consumer_group = "telegraf_metrics_consumers"
#
#   ## Compression codec represents the various compression codecs recognized by
#   ## Kafka in messages.
#   ##  0 : None
#   ##  1 : Gzip
#   ##  2 : Snappy
#   ##  3 : LZ4
#   ##  4 : ZSTD
#   # compression_codec = 0
#   ## Initial offset position; one of "oldest" or "newest".
#   # offset = "oldest"
#
#   ## Consumer group partition assignment strategy; one of "range", "roundrobin" or "sticky".
#   # balance_strategy = "range"
#
#   ## Maximum number of retries for metadata operations including
#   ## connecting. Sets Sarama library's Metadata.Retry.Max config value. If 0 or
#   ## unset, use the Sarama default of 3,
#   # metadata_retry_max = 0
#
#   ## Type of retry backoff. Valid options: "constant", "exponential"
#   # metadata_retry_type = "constant"
#
#   ## Amount of time to wait before retrying. When metadata_retry_type is
#   ## "constant", each retry is delayed this amount. When "exponential", the
#   ## first retry is delayed this amount, and subsequent delays are doubled. If 0
#   ## or unset, use the Sarama default of 250 ms
#   # metadata_retry_backoff = 0
#
#   ## Maximum amount of time to wait before retrying when metadata_retry_type is
#   ## "exponential". Ignored for other retry types. If 0, there is no backoff
#   ## limit.
#   # metadata_retry_max_duration = 0
#
#   ## Strategy for making connection to kafka brokers. Valid options: "startup",
#   ## "defer". If set to "defer" the plugin is allowed to start before making a
#   ## connection. This is useful if the broker may be down when telegraf is
#   ## started, but if there are any typos in the broker setting, they will cause
#   ## connection failures without warning at startup
#   # connection_strategy = "startup"
#
#   ## Maximum length of a message to consume, in bytes (default 0/unlimited);
#   ## larger messages are dropped
#   max_message_len = 1000000
#
#   ## Max undelivered messages
#   ## This plugin uses tracking metrics, which ensure messages are read to
#   ## outputs before acknowledging them to the original broker to ensure data
#   ## is not lost. This option sets the maximum messages to read from the
#   ## broker that have not been written by an output.
#   ##
#   ## This value needs to be picked with awareness of the agent's
#   ## metric_batch_size value as well. Setting max undelivered messages too high
#   ## can result in a constant stream of data batches to the output. While
#   ## setting it too low may never flush the broker's messages.
#   # max_undelivered_messages = 1000
#
#   ## Maximum amount of time the consumer should take to process messages. If
#   ## the debug log prints messages from sarama about 'abandoning subscription
#   ## to [topic] because consuming was taking too long', increase this value to
#   ## longer than the time taken by the output plugin(s).
#   ##
#   ## Note that the effective timeout could be between 'max_processing_time' and
#   ## '2 * max_processing_time'.
#   # max_processing_time = "100ms"
#
#   ## The default number of message bytes to fetch from the broker in each
#   ## request (default 1MB). This should be larger than the majority of
#   ## your messages, or else the consumer will spend a lot of time
#   ## negotiating sizes and not actually consuming. Similar to the JVM's
#   ## `fetch.message.max.bytes`.
#   # consumer_fetch_default = "1MB"
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"


# # Configuration for the AWS Kinesis input.
# [[inputs.kinesis_consumer]]
#   ## Amazon REGION of kinesis endpoint.
#   region = "ap-southeast-2"
#
#   ## Amazon Credentials
#   ## Credentials are loaded in the following order
#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified
#   ## 2) Assumed credentials via STS if role_arn is specified
#   ## 3) explicit credentials from 'access_key' and 'secret_key'
#   ## 4) shared profile from 'profile'
#   ## 5) environment variables
#   ## 6) shared credentials file
#   ## 7) EC2 Instance Profile
#   # access_key = ""
#   # secret_key = ""
#   # token = ""
#   # role_arn = ""
#   # web_identity_token_file = ""
#   # role_session_name = ""
#   # profile = ""
#   # shared_credential_file = ""
#
#   ## Endpoint to make request against, the correct endpoint is automatically
#   ## determined and this option should only be set if you wish to override the
#   ## default.
#   ##   ex: endpoint_url = "http://localhost:8000"
#   # endpoint_url = ""
#
#   ## Kinesis StreamName must exist prior to starting telegraf.
#   streamname = "StreamName"
#
#   ## Shard iterator type (only 'TRIM_HORIZON' and 'LATEST' currently supported)
#   # shard_iterator_type = "TRIM_HORIZON"
#
#   ## Max undelivered messages
#   ## This plugin uses tracking metrics, which ensure messages are read to
#   ## outputs before acknowledging them to the original broker to ensure data
#   ## is not lost. This option sets the maximum messages to read from the
#   ## broker that have not been written by an output.
#   ##
#   ## This value needs to be picked with awareness of the agent's
#   ## metric_batch_size value as well. Setting max undelivered messages too high
#   ## can result in a constant stream of data batches to the output. While
#   ## setting it too low may never flush the broker's messages.
#   # max_undelivered_messages = 1000
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"
#
#   ##
#   ## The content encoding of the data from kinesis
#   ## If you are processing a cloudwatch logs kinesis stream then set this to "gzip"
#   ## as AWS compresses cloudwatch log data before it is sent to kinesis (aws
#   ## also base64 encodes the zip byte data before pushing to the stream.  The base64 decoding
#   ## is done automatically by the golang sdk, as data is read from kinesis)
#   ##
#   # content_encoding = "identity"
#
#   ## Optional
#   ## Configuration for a dynamodb checkpoint
#   [inputs.kinesis_consumer.checkpoint_dynamodb]
#     ## unique name for this consumer
#     app_name = "default"
#     table_name = "default"



# # Read metrics from NATS subject(s)
# [[inputs.nats_consumer]]
#   ## urls of NATS servers
#   servers = ["nats://localhost:4222"]
#
#   ## subject(s) to consume
#   ## If you use jetstream you need to set the subjects
#   ## in jetstream_subjects
#   subjects = ["telegraf"]
#
#   ## jetstream subjects
#   ## jetstream is a streaming technology inside of nats.
#   ## With jetstream the nats-server persists messages and
#   ## a consumer can consume historical messages. This is
#   ## useful when telegraf needs to restart it don't miss a
#   ## message. You need to configure the nats-server.
#   ## https://docs.nats.io/nats-concepts/jetstream.
#   jetstream_subjects = ["js_telegraf"]
#
#   ## name a queue group
#   queue_group = "telegraf_consumers"
#
#   ## Optional credentials
#   # username = ""
#   # password = ""
#
#   ## Optional NATS 2.0 and NATS NGS compatible user credentials
#   # credentials = "/etc/telegraf/nats.creds"
#
#   ## Use Transport Layer Security
#   # secure = false
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Sets the limits for pending msgs and bytes for each subscription
#   ## These shouldn't need to be adjusted except in very high throughput scenarios
#   # pending_message_limit = 65536
#   # pending_bytes_limit = 67108864
#
#   ## Max undelivered messages
#   ## This plugin uses tracking metrics, which ensure messages are read to
#   ## outputs before acknowledging them to the original broker to ensure data
#   ## is not lost. This option sets the maximum messages to read from the
#   ## broker that have not been written by an output.
#   ##
#   ## This value needs to be picked with awareness of the agent's
#   ## metric_batch_size value as well. Setting max undelivered messages too high
#   ## can result in a constant stream of data batches to the output. While
#   ## setting it too low may never flush the broker's messages.
#   # max_undelivered_messages = 1000
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"


# # Netflow v5, Netflow v9 and IPFIX collector
# [[inputs.netflow]]
#   ## Address to listen for netflow,ipfix or sflow packets.
#   ##   example: service_address = "udp://:2055"
#   ##            service_address = "udp4://:2055"
#   ##            service_address = "udp6://:2055"
#   service_address = "udp://:2055"
#
#   ## Set the size of the operating system's receive buffer.
#   ##   example: read_buffer_size = "64KiB"
#   ## Uses the system's default if not set.
#   # read_buffer_size = ""
#
#   ## Protocol version to use for decoding.
#   ## Available options are
#   ##   "ipfix"      -- IPFIX / Netflow v10 protocol (also works for Netflow v9)
#   ##   "netflow v5" -- Netflow v5 protocol
#   ##   "netflow v9" -- Netflow v9 protocol (also works for IPFIX)
#   ##   "sflow v5"   -- sFlow v5 protocol
#   # protocol = "ipfix"
#
#   ## Private Enterprise Numbers (PEN) mappings for decoding
#   ## This option allows to specify vendor-specific mapping files to use during
#   ## decoding.
#   # private_enterprise_number_files = []
#
#   ## Dump incoming packets to the log
#   ## This can be helpful to debug parsing issues. Only active if
#   ## Telegraf is in debug mode.
#   # dump_packets = false


# # Read metrics from NSQD topic(s)
# [[inputs.nsq_consumer]]
#   ## Server option still works but is deprecated, we just prepend it to the nsqd array.
#   # server = "localhost:4150"
#
#   ## An array representing the NSQD TCP HTTP Endpoints
#   nsqd = ["localhost:4150"]
#
#   ## An array representing the NSQLookupd HTTP Endpoints
#   nsqlookupd = ["localhost:4161"]
#   topic = "telegraf"
#   channel = "consumer"
#   max_in_flight = 100
#
#   ## Max undelivered messages
#   ## This plugin uses tracking metrics, which ensure messages are read to
#   ## outputs before acknowledging them to the original broker to ensure data
#   ## is not lost. This option sets the maximum messages to read from the
#   ## broker that have not been written by an output.
#   ##
#   ## This value needs to be picked with awareness of the agent's
#   ## metric_batch_size value as well. Setting max undelivered messages too high
#   ## can result in a constant stream of data batches to the output. While
#   ## setting it too low may never flush the broker's messages.
#   # max_undelivered_messages = 1000
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"


# # Retrieve data from OPCUA devices
# [[inputs.opcua_listener]]
#   ## Metric name
#   # name = "opcua_listener"
#   #
#   ## OPC UA Endpoint URL
#   # endpoint = "opc.tcp://localhost:4840"
#   #
#   ## Maximum time allowed to establish a connect to the endpoint.
#   # connect_timeout = "10s"
#   #
#   ## Maximum time allowed for a request over the established connection.
#   # request_timeout = "5s"
#   #
#   ## The interval at which the server should at least update its monitored items
#   # subscription_interval = "100ms"
#   #
#   ## Security policy, one of "None", "Basic128Rsa15", "Basic256",
#   ## "Basic256Sha256", or "auto"
#   # security_policy = "auto"
#   #
#   ## Security mode, one of "None", "Sign", "SignAndEncrypt", or "auto"
#   # security_mode = "auto"
#   #
#   ## Path to cert.pem. Required when security mode or policy isn't "None".
#   ## If cert path is not supplied, self-signed cert and key will be generated.
#   # certificate = "/etc/telegraf/cert.pem"
#   #
#   ## Path to private key.pem. Required when security mode or policy isn't "None".
#   ## If key path is not supplied, self-signed cert and key will be generated.
#   # private_key = "/etc/telegraf/key.pem"
#   #
#   ## Authentication Method, one of "Certificate", "UserName", or "Anonymous".  To
#   ## authenticate using a specific ID, select 'Certificate' or 'UserName'
#   # auth_method = "Anonymous"
#   #
#   ## Username. Required for auth_method = "UserName"
#   # username = ""
#   #
#   ## Password. Required for auth_method = "UserName"
#   # password = ""
#   #
#   ## Option to select the metric timestamp to use. Valid options are:
#   ##     "gather" -- uses the time of receiving the data in telegraf
#   ##     "server" -- uses the timestamp provided by the server
#   ##     "source" -- uses the timestamp provided by the source
#   # timestamp = "gather"
#   #
#   ## The default timetsamp format is RFC3339Nano
#   # Other timestamp layouts can be configured using the Go language time
#   # layout specification from https://golang.org/pkg/time/#Time.Format
#   # e.g.: json_timestamp_format = "2006-01-02T15:04:05Z07:00"
#   #timestamp_format = ""
#   #
#   ## Node ID configuration
#   ## name              - field name to use in the output
#   ## namespace         - OPC UA namespace of the node (integer value 0 thru 3)
#   ## identifier_type   - OPC UA ID type (s=string, i=numeric, g=guid, b=opaque)
#   ## identifier        - OPC UA ID (tag as shown in opcua browser)
#   ## default_tags      - extra tags to be added to the output metric (optional)
#   ##
#   ## Use either the inline notation or the bracketed notation, not both.
#   #
#   ## Inline notation (default_tags not supported yet)
#   # nodes = [
#   #   {name="", namespace="", identifier_type="", identifier=""},
#   #   {name="", namespace="", identifier_type="", identifier=""},
#   # ]
#   #
#   ## Bracketed notation
#   # [[inputs.opcua_listener.nodes]]
#   #   name = "node1"
#   #   namespace = ""
#   #   identifier_type = ""
#   #   identifier = ""
#   #   default_tags = { tag1 = "value1", tag2 = "value2" }
#   #
#   # [[inputs.opcua_listener.nodes]]
#   #   name = "node2"
#   #   namespace = ""
#   #   identifier_type = ""
#   #   identifier = ""
#   #
#   ## Node Group
#   ## Sets defaults so they aren't required in every node.
#   ## Default values can be set for:
#   ## * Metric name
#   ## * OPC UA namespace
#   ## * Identifier
#   ## * Default tags
#   ##
#   ## Multiple node groups are allowed
#   #[[inputs.opcua_listener.group]]
#   ## Group Metric name. Overrides the top level name.  If unset, the
#   ## top level name is used.
#   # name =
#   #
#   ## Group default namespace. If a node in the group doesn't set its
#   ## namespace, this is used.
#   # namespace =
#   #
#   ## Group default identifier type. If a node in the group doesn't set its
#   ## namespace, this is used.
#   # identifier_type =
#   #
#   ## Default tags that are applied to every node in this group. Can be
#   ## overwritten in a node by setting a different value for the tag name.
#   ##   example: default_tags = { tag1 = "value1" }
#   # default_tags = {}
#   #
#   ## Node ID Configuration.  Array of nodes with the same settings as above.
#   ## Use either the inline notation or the bracketed notation, not both.
#   #
#   ## Inline notation (default_tags not supported yet)
#   # nodes = [
#   #  {name="node1", namespace="", identifier_type="", identifier=""},
#   #  {name="node2", namespace="", identifier_type="", identifier=""},
#   #]
#   #
#   ## Bracketed notation
#   # [[inputs.opcua_listener.group.nodes]]
#   #   name = "node1"
#   #   namespace = ""
#   #   identifier_type = ""
#   #   identifier = ""
#   #   default_tags = { tag1 = "override1", tag2 = "value2" }
#   #
#   # [[inputs.opcua_listener.group.nodes]]
#   #   name = "node2"
#   #   namespace = ""
#   #   identifier_type = ""
#   #   identifier = ""
#
#   ## Enable workarounds required by some devices to work correctly
#   # [inputs.opcua_listener.workarounds]
#     ## Set additional valid status codes, StatusOK (0x0) is always considered valid
#     # additional_valid_status_codes = ["0xC0"]
#
#   # [inputs.opcua_listener.request_workarounds]
#     ## Use unregistered reads instead of registered reads
#     # use_unregistered_reads = false


# # Receive OpenTelemetry traces, metrics, and logs over gRPC
# [[inputs.opentelemetry]]
#   ## Override the default (0.0.0.0:4317) destination OpenTelemetry gRPC service
#   ## address:port
#   # service_address = "0.0.0.0:4317"
#
#   ## Override the default (5s) new connection timeout
#   # timeout = "5s"
#
#   ## Override the default span attributes to be used as line protocol tags.
#   ## These are always included as tags:
#   ## - trace ID
#   ## - span ID
#   ## The default values are strongly recommended for use with Jaeger:
#   ## - service.name
#   ## - span.name
#   ## Other common attributes can be found here:
#   ## - https://github.com/open-telemetry/opentelemetry-collector/tree/main/semconv
#   # span_dimensions = ["service.name", "span.name"]
#
#   ## Override the default (prometheus-v1) metrics schema.
#   ## Supports: "prometheus-v1", "prometheus-v2"
#   ## For more information about the alternatives, read the Prometheus input
#   ## plugin notes.
#   # metrics_schema = "prometheus-v1"
#
#   ## Optional TLS Config.
#   ## For advanced options: https://github.com/influxdata/telegraf/blob/v1.18.3/docs/TLS.md
#   ##
#   ## Set one or more allowed client CA certificate file names to
#   ## enable mutually authenticated TLS connections.
#   # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#   ## Add service certificate and key.
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"


# # Read metrics from one or many postgresql servers
# [[inputs.postgresql]]
#   ## Specify address via a url matching:
#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]&statement_timeout=...
#   ## or a simple string:
#   ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production
#   ## Users can pass the path to the socket as the host value to use a socket
#   ## connection (e.g. `/var/run/postgresql`).
#   ##
#   ## All connection parameters are optional.
#   ##
#   ## Without the dbname parameter, the driver will default to a database
#   ## with the same name as the user. This dbname is just for instantiating a
#   ## connection with the server and doesn't restrict the databases we are trying
#   ## to grab metrics for.
#   ##
#   address = "host=localhost user=postgres sslmode=disable"
#
#   ## A custom name for the database that will be used as the "server" tag in the
#   ## measurement output. If not specified, a default one generated from
#   ## the connection address is used.
#   # outputaddress = "db01"
#
#   ## connection configuration.
#   ## maxlifetime - specify the maximum lifetime of a connection.
#   ## default is forever (0s)
#   ##
#   ## Note that this does not interrupt queries, the lifetime will not be enforced
#   ## whilst a query is running
#   # max_lifetime = "0s"
#
#   ## A  list of databases to explicitly ignore.  If not specified, metrics for all
#   ## databases are gathered.  Do NOT use with the 'databases' option.
#   # ignored_databases = ["postgres", "template0", "template1"]
#
#   ## A list of databases to pull metrics about. If not specified, metrics for all
#   ## databases are gathered.  Do NOT use with the 'ignored_databases' option.
#   # databases = ["app_production", "testing"]
#
#   ## Whether to use prepared statements when connecting to the database.
#   ## This should be set to false when connecting through a PgBouncer instance
#   ## with pool_mode set to transaction.
#   prepared_statements = true



# # Read metrics from one or many prometheus clients
# [[inputs.prometheus]]
#   ## An array of urls to scrape metrics from.
#   urls = ["http://localhost:9100/metrics"]
#
#   ## Metric version controls the mapping from Prometheus metrics into Telegraf metrics.
#   ## See "Metric Format Configuration" in plugins/inputs/prometheus/README.md for details.
#   ## Valid options: 1, 2
#   # metric_version = 1
#
#   ## Url tag name (tag containing scrapped url. optional, default is "url")
#   # url_tag = "url"
#
#   ## Whether the timestamp of the scraped metrics will be ignored.
#   ## If set to true, the gather time will be used.
#   # ignore_timestamp = false
#
#   ## An array of Kubernetes services to scrape metrics from.
#   # kubernetes_services = ["http://my-service-dns.my-namespace:9100/metrics"]
#
#   ## Kubernetes config file to create client from.
#   # kube_config = "/path/to/kubernetes.config"
#
#   ## Scrape Pods
#   ## Enable scraping of k8s pods. Further settings as to which pods to scape
#   ## are determiend by the 'method' option below. When enabled, the default is
#   ## to use annotations to determine whether to scrape or not.
#   # monitor_kubernetes_pods = false
#
#   ## Scrape Pods Method
#   ## annotations: default, looks for specific pod annotations documented below
#   ## settings: only look for pods matching the settings provided, not
#   ##   annotations
#   ## settings+annotations: looks at pods that match annotations using the user
#   ##   defined settings
#   # monitor_kubernetes_pods_method = "annotations"
#
#   ## Scrape Pods 'annotations' method options
#   ## If set method is set to 'annotations' or 'settings+annotations', these
#   ## annotation flags are looked for:
#   ## - prometheus.io/scrape: Required to enable scraping for this pod. Can also
#   ##     use 'prometheus.io/scrape=false' annotation to opt-out entirely.
#   ## - prometheus.io/scheme: If the metrics endpoint is secured then you will
#   ##     need to set this to 'https' & most likely set the tls config
#   ## - prometheus.io/path: If the metrics path is not /metrics, define it with
#   ##     this annotation
#   ## - prometheus.io/port: If port is not 9102 use this annotation
#
#   ## Scrape Pods 'settings' method options
#   ## When using 'settings' or 'settings+annotations', the default values for
#   ## annotations can be modified using with the following options:
#   # monitor_kubernetes_pods_scheme = "http"
#   # monitor_kubernetes_pods_port = "9102"
#   # monitor_kubernetes_pods_path = "/metrics"
#
#   ## Get the list of pods to scrape with either the scope of
#   ## - cluster: the kubernetes watch api (default, no need to specify)
#   ## - node: the local cadvisor api; for scalability. Note that the config node_ip or the environment variable NODE_IP must be set to the host IP.
#   # pod_scrape_scope = "cluster"
#
#   ## Only for node scrape scope: node IP of the node that telegraf is running on.
#   ## Either this config or the environment variable NODE_IP must be set.
#   # node_ip = "10.180.1.1"
#
#   ## Only for node scrape scope: interval in seconds for how often to get updated pod list for scraping.
#   ## Default is 60 seconds.
#   # pod_scrape_interval = 60
#
#   ## Restricts Kubernetes monitoring to a single namespace
#   ##   ex: monitor_kubernetes_pods_namespace = "default"
#   # monitor_kubernetes_pods_namespace = ""
#   ## The name of the label for the pod that is being scraped.
#   ## Default is 'namespace' but this can conflict with metrics that have the label 'namespace'
#   # pod_namespace_label_name = "namespace"
#   # label selector to target pods which have the label
#   # kubernetes_label_selector = "env=dev,app=nginx"
#   # field selector to target pods
#   # eg. To scrape pods on a specific node
#   # kubernetes_field_selector = "spec.nodeName=$HOSTNAME"
#
#   ## Filter which pod annotations and labels will be added to metric tags
#   #
#   # pod_annotation_include = ["annotation-key-1"]
#   # pod_annotation_exclude = ["exclude-me"]
#   # pod_label_include = ["label-key-1"]
#   # pod_label_exclude = ["exclude-me"]
#
#   # cache refresh interval to set the interval for re-sync of pods list.
#   # Default is 60 minutes.
#   # cache_refresh_interval = 60
#
#   ## Scrape Services available in Consul Catalog
#   # [inputs.prometheus.consul]
#   #   enabled = true
#   #   agent = "http://localhost:8500"
#   #   query_interval = "5m"
#
#   #   [[inputs.prometheus.consul.query]]
#   #     name = "a service name"
#   #     tag = "a service tag"
#   #     url = 'http://{{if ne .ServiceAddress ""}}{{.ServiceAddress}}{{else}}{{.Address}}{{end}}:{{.ServicePort}}/{{with .ServiceMeta.metrics_path}}{{.}}{{else}}metrics{{end}}'
#   #     [inputs.prometheus.consul.query.tags]
#   #       host = "{{.Node}}"
#
#   ## Use bearer token for authorization. ('bearer_token' takes priority)
#   # bearer_token = "/path/to/bearer/token"
#   ## OR
#   # bearer_token_string = "abc_123"
#
#   ## HTTP Basic Authentication username and password. ('bearer_token' and
#   ## 'bearer_token_string' take priority)
#   # username = ""
#   # password = ""
#
#   ## Optional custom HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## Specify timeout duration for slower prometheus clients (default is 5s)
#   # timeout = "5s"
#
#   ## deprecated in 1.26; use the timeout option
#   # response_timeout = "5s"
#
#   ## HTTP Proxy support
#   # use_system_proxy = false
#   # http_proxy_url = ""
#
#   ## Optional TLS Config
#   # tls_ca = /path/to/cafile
#   # tls_cert = /path/to/certfile
#   # tls_key = /path/to/keyfile
#
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Use the given name as the SNI server name on each URL
#   # tls_server_name = "myhost.example.org"
#
#   ## TLS renegotiation method, choose from "never", "once", "freely"
#   # tls_renegotiation_method = "never"
#
#   ## Enable/disable TLS
#   ## Set to true/false to enforce TLS being enabled/disabled. If not set,
#   ## enable TLS only if any of the other options are specified.
#   # tls_enable = true
#
#   ## Control pod scraping based on pod namespace annotations
#   ## Pass and drop here act like tagpass and tagdrop, but instead
#   ## of filtering metrics they filters pod candidates for scraping
#   #[inputs.prometheus.namespace_annotation_pass]
#   # annotation_key = ["value1", "value2"]
#   #[inputs.prometheus.namespace_annotation_drop]
#   # some_annotation_key = ["dont-scrape"]
